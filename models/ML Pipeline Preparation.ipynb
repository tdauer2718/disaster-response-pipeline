{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "import re\n",
    "from time import time, perf_counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tjdauer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tjdauer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tjdauer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download packages from NLTK\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the database, data.db\n",
    "engine = create_engine('sqlite:///../data/data.db')\n",
    "df = pd.read_sql(\"SELECT * FROM LabeledMessages\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the series of messages X and the target matrix Y\n",
    "X = df['message']\n",
    "Y = df.drop(columns=['id', 'message', 'original', 'genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "2                      Looking for someone but no name\n",
       "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
       "4    says: west side of Haiti, rest of the country ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few entries of X\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>child_alone</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "3        1        1      0            1             0                 1   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  child_alone  ...  aid_centers  \\\n",
       "0                  0         0         0            0  ...            0   \n",
       "1                  0         0         0            0  ...            0   \n",
       "2                  0         0         0            0  ...            0   \n",
       "3                  0         0         0            0  ...            0   \n",
       "4                  0         0         0            0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of Y\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for tokenization and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Takes some text as input, then tokenizes it, lemmatizes it, strips it\n",
    "        of characters that are not letters or numbers, and normalizes its case \n",
    "        (to lowercase only).\n",
    "        \n",
    "    Arguments:\n",
    "    text -- a string of text.\n",
    "    \n",
    "    Returns:\n",
    "    tokens_cleaned -- the final list of cleaned tokens.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z0-9]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_cleaned = [lemmatizer.lemmatize(t).strip() for t in tokens]\n",
    "    return tokens_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'thanked', 'm', 'smith', 'she', 'is', 'a', 'great', 'teacher']\n"
     ]
    }
   ],
   "source": [
    "# Use the function on an example sentence\n",
    "print(tokenize('I thanked Ms. Smith. She is a great teacher.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stopwords that are consistent with my tokenizer\n",
    "my_stopwords = set([tokenize(w)[0] for w in stopwords.words('english')])\n",
    "# I used `set` to delete duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from', 'wa', 'isn', 'again', 'through', 'then', 'all', 'shouldn', 'there', 'doing', 'while', 'wasn', 'those', 'weren', 'against', 'his', 'very', 'her', 'but', 'their', 'have', 'ha', 'further', 'hasn', 'wouldn', 'yours', 'of', 'ours', 'once', 'y', 'me', 'is', 't', 'now', 'other', 'ma', 'some', 'between', 'at', 'had', 'having', 'until', 'same', 'she', 'him', 'out', 'doe', 'mustn', 'we', 'shan', 'about', 'too', 'not', 'didn', 'haven', 'hers', 'if', 'them', 'be', 've', 'do', 'your', 'for', 'an', 'because', 'only', 'he', 'my', 'i', 'theirs', 'such', 'yourself', 'above', 'being', 'a', 'each', 'in', 's', 'ain', 'just', 'under', 'by', 'will', 'hadn', 're', 'below', 'down', 'nor', 'should', 'both', 'they', 'which', 'don', 'needn', 'with', 'or', 'this', 'before', 'that', 'when', 'why', 'into', 'no', 'after', 'll', 'whom', 'these', 'herself', 'itself', 'you', 'here', 'm', 'up', 'to', 'the', 'where', 'been', 'during', 'most', 'any', 'on', 'over', 'our', 'himself', 'than', 'can', 'themselves', 'and', 'own', 'o', 'couldn', 'so', 'won', 'were', 'more', 'what', 'doesn', 'ourselves', 'how', 'mightn', 'did', 'am', 'who', 'yourselves', 'aren', 'off', 'it', 'are', 'd', 'myself', 'few'}\n"
     ]
    }
   ],
   "source": [
    "# Print the stopwords\n",
    "print(my_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to create a machine learning pipeline to predict category values given new messages. But first, there's an issue with the data that I need to deal with. Some of the messages become the empty string after applying the tokenizer and removing stopwords; these will cause an error when using `CountVectorizer`. I will therefore check for such messages explicitly and remove the corresponding rows. I'll do this by making each message into a set of tokenized words, subtracting the set of stopwords (subtracting as sets is faster than using a list-based approach), and then seeing if the result is the empty set. Alternatively, I could use `try`-`except` to see on which rows the `CountVectorizer` with my tokenizer and stop words fails, but that approach is rather wasteful since I don't need to do vectorization on rows that don't return the empty set after tokenization and stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the result after tokenization and stopword removal on the messages\n",
    "X_check = X.apply(lambda x: set(tokenize(x)) - my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12031    {}\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the resulting rows\n",
    "failed_rows = X_check[X_check == set()].index\n",
    "X_check[failed_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the appropriate row(s)\n",
    "X.drop(failed_rows, inplace=True)\n",
    "Y.drop(failed_rows, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create two pipelines. They both apply a `CountVectorizer` first and then a TF.IDF transformer. For one of them, I then feed this into a different Random Forest Classifier for each column of $Y$ using `MultiOutputClassifier`. For the other, I use `RandomForestClassifier`'s built-in ability to handle multiple outputs, training a single model to simultaneously predict all the outputs (columns of $Y$). For the former, 35 models are independently trained (one for each column of $Y$), but for the latter only one model is trained. As explained [here](https://scikit-learn.org/stable/modules/tree.html#tree-multioutput), advantages of the second approach are shorter training time and typically improved generalization accuracy.\n",
    "\n",
    "Below, I create, train, and make predictions using both pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline that uses a CountVectorizer with my above tokenizer\n",
    "# function, then uses a Tf-idf transformer, then uses an independent\n",
    "# random forest classifier for each output \n",
    "# (utilizing MultiOutputClassifier())\n",
    "pipe_multi = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, stop_words=my_stopwords)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=42))),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same first two steps as in the previous cell, but now use a \n",
    "# single random forest classifier in the last step\n",
    "pipe_single = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenize, stop_words=my_stopwords)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier(random_state=42)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll split the data into train and test sets, then fit the model to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a train-test split, with 30% of the data treated as test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time = 160 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fit the first pipeline to the train data\n",
    "t0 = time()\n",
    "pipe_multi.fit(X_train, Y_train)\n",
    "t1 = time()\n",
    "print(f'Fitting time = {t1 - t0:.3g} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time = 40.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# Fit the second pipeline to the train data\n",
    "t0 = time()\n",
    "pipe_single.fit(X_train, Y_train)\n",
    "t1 = time()\n",
    "print(f'Fitting time = {t1 - t0:.3g} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline takes about 4 times as long to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll use the fitted models to make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time = 39.8 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use the first fitted pipeline\n",
    "t0 = time()\n",
    "Y_pred_multi = pipe_multi.predict(X_test)\n",
    "t1 = time()\n",
    "print(f'Prediction time = {t1 - t0:.3g} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time = 3.14 seconds\n"
     ]
    }
   ],
   "source": [
    "# Use the second fitted pipeline\n",
    "t0 = time()\n",
    "Y_pred_single = pipe_single.predict(X_test)\n",
    "t1 = time()\n",
    "print(f'Prediction time = {t1 - t0:.3g} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline takes more than 10 times as long to make predictions than the second does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27148162376744783"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the `score` method to find the score of the first fitted pipeline\n",
    "pipe_multi.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure how this score is calculated. According to Scikit-Learn's documentation, the default scorer for the pipeline is the same as that for the last esimator, in this case a random forest classifier. But `RandomForestClassifier()`'s scoring metric is the mean accuracy, and as we'll see below, the accuracy reported by `classification_report` is significantly higher than 0.27 for every column of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll print a classification report for each pipeline, reporting various metrics measured on the test set predictions versus true values. I'll print the report so that for each target variable, the first pipeline's report and then the second's are printed consecutively; that way they can be more easily compared at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Y.columns).index('child_alone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(Y_test['child_alone'], Y_pred_single[:, 9], zero_division=0,\n",
    "                              output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'0' in report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key error\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    report['1']\n",
    "except KeyError:\n",
    "    print('Key error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mrelated:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.39      0.51      1844\n",
      "           1       0.84      0.95      0.89      5965\n",
      "\n",
      "    accuracy                           0.82      7809\n",
      "   macro avg       0.78      0.67      0.70      7809\n",
      "weighted avg       0.81      0.82      0.80      7809\n",
      "\n",
      "\u001b[1mrelated:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.54      0.60      1844\n",
      "           1       0.87      0.92      0.89      5965\n",
      "\n",
      "    accuracy                           0.83      7809\n",
      "   macro avg       0.78      0.73      0.75      7809\n",
      "weighted avg       0.82      0.83      0.83      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mrequest:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      6474\n",
      "           1       0.81      0.50      0.62      1335\n",
      "\n",
      "    accuracy                           0.89      7809\n",
      "   macro avg       0.86      0.74      0.78      7809\n",
      "weighted avg       0.89      0.89      0.88      7809\n",
      "\n",
      "\u001b[1mrequest:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      6474\n",
      "           1       0.80      0.51      0.62      1335\n",
      "\n",
      "    accuracy                           0.89      7809\n",
      "   macro avg       0.85      0.74      0.78      7809\n",
      "weighted avg       0.89      0.89      0.88      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1moffer:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7766\n",
      "           1       0.00      0.00      0.00        43\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.50      0.50      0.50      7809\n",
      "weighted avg       0.99      0.99      0.99      7809\n",
      "\n",
      "\u001b[1moffer:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7766\n",
      "           1       0.00      0.00      0.00        43\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.50      0.50      0.50      7809\n",
      "weighted avg       0.99      0.99      0.99      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1maid_related:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      4573\n",
      "           1       0.77      0.71      0.74      3236\n",
      "\n",
      "    accuracy                           0.79      7809\n",
      "   macro avg       0.79      0.78      0.78      7809\n",
      "weighted avg       0.79      0.79      0.79      7809\n",
      "\n",
      "\u001b[1maid_related:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.91      0.82      4573\n",
      "           1       0.81      0.55      0.66      3236\n",
      "\n",
      "    accuracy                           0.76      7809\n",
      "   macro avg       0.77      0.73      0.74      7809\n",
      "weighted avg       0.77      0.76      0.75      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmedical_help:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      7191\n",
      "           1       0.75      0.07      0.12       618\n",
      "\n",
      "    accuracy                           0.92      7809\n",
      "   macro avg       0.84      0.53      0.54      7809\n",
      "weighted avg       0.91      0.92      0.89      7809\n",
      "\n",
      "\u001b[1mmedical_help:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96      7191\n",
      "           1       0.58      0.02      0.03       618\n",
      "\n",
      "    accuracy                           0.92      7809\n",
      "   macro avg       0.75      0.51      0.50      7809\n",
      "weighted avg       0.89      0.92      0.89      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmedical_products:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      7414\n",
      "           1       0.80      0.07      0.13       395\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.88      0.53      0.55      7809\n",
      "weighted avg       0.95      0.95      0.93      7809\n",
      "\n",
      "\u001b[1mmedical_products:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7414\n",
      "           1       0.80      0.03      0.06       395\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.88      0.51      0.52      7809\n",
      "weighted avg       0.94      0.95      0.93      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1msearch_and_rescue:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      7588\n",
      "           1       0.82      0.04      0.08       221\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.90      0.52      0.53      7809\n",
      "weighted avg       0.97      0.97      0.96      7809\n",
      "\n",
      "\u001b[1msearch_and_rescue:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      7588\n",
      "           1       0.71      0.02      0.04       221\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.84      0.51      0.51      7809\n",
      "weighted avg       0.97      0.97      0.96      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1msecurity:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7651\n",
      "           1       1.00      0.01      0.01       158\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.99      0.50      0.50      7809\n",
      "weighted avg       0.98      0.98      0.97      7809\n",
      "\n",
      "\u001b[1msecurity:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7651\n",
      "           1       1.00      0.01      0.01       158\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.99      0.50      0.50      7809\n",
      "weighted avg       0.98      0.98      0.97      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmilitary:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      7560\n",
      "           1       0.70      0.06      0.12       249\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.83      0.53      0.55      7809\n",
      "weighted avg       0.96      0.97      0.96      7809\n",
      "\n",
      "\u001b[1mmilitary:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      7560\n",
      "           1       0.46      0.02      0.05       249\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.72      0.51      0.51      7809\n",
      "weighted avg       0.95      0.97      0.95      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mwater:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7315\n",
      "           1       0.91      0.38      0.54       494\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.94      0.69      0.76      7809\n",
      "weighted avg       0.96      0.96      0.95      7809\n",
      "\n",
      "\u001b[1mwater:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7315\n",
      "           1       0.93      0.25      0.40       494\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.94      0.63      0.69      7809\n",
      "weighted avg       0.95      0.95      0.94      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfood:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97      6934\n",
      "           1       0.82      0.61      0.70       875\n",
      "\n",
      "    accuracy                           0.94      7809\n",
      "   macro avg       0.89      0.80      0.84      7809\n",
      "weighted avg       0.94      0.94      0.94      7809\n",
      "\n",
      "\u001b[1mfood:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      6934\n",
      "           1       0.85      0.48      0.61       875\n",
      "\n",
      "    accuracy                           0.93      7809\n",
      "   macro avg       0.90      0.73      0.79      7809\n",
      "weighted avg       0.93      0.93      0.92      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mshelter:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97      7105\n",
      "           1       0.81      0.39      0.53       704\n",
      "\n",
      "    accuracy                           0.94      7809\n",
      "   macro avg       0.88      0.69      0.75      7809\n",
      "weighted avg       0.93      0.94      0.93      7809\n",
      "\n",
      "\u001b[1mshelter:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      7105\n",
      "           1       0.83      0.24      0.38       704\n",
      "\n",
      "    accuracy                           0.93      7809\n",
      "   macro avg       0.88      0.62      0.67      7809\n",
      "weighted avg       0.92      0.93      0.91      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mclothing:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7686\n",
      "           1       0.80      0.10      0.17       123\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.89      0.55      0.58      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "\u001b[1mclothing:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7686\n",
      "           1       0.86      0.10      0.18       123\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.92      0.55      0.58      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmoney:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7622\n",
      "           1       0.82      0.05      0.09       187\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.90      0.52      0.54      7809\n",
      "weighted avg       0.97      0.98      0.97      7809\n",
      "\n",
      "\u001b[1mmoney:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7622\n",
      "           1       0.80      0.04      0.08       187\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.89      0.52      0.53      7809\n",
      "weighted avg       0.97      0.98      0.97      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmissing_people:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7730\n",
      "           1       1.00      0.01      0.02        79\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       1.00      0.51      0.51      7809\n",
      "weighted avg       0.99      0.99      0.99      7809\n",
      "\n",
      "\u001b[1mmissing_people:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7730\n",
      "           1       0.00      0.00      0.00        79\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mrefugees:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      7561\n",
      "           1       0.75      0.02      0.05       248\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.86      0.51      0.52      7809\n",
      "weighted avg       0.96      0.97      0.95      7809\n",
      "\n",
      "\u001b[1mrefugees:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      7561\n",
      "           1       0.38      0.01      0.02       248\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.67      0.51      0.50      7809\n",
      "weighted avg       0.95      0.97      0.95      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mdeath:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7450\n",
      "           1       0.86      0.18      0.30       359\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.91      0.59      0.64      7809\n",
      "weighted avg       0.96      0.96      0.95      7809\n",
      "\n",
      "\u001b[1mdeath:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7450\n",
      "           1       0.96      0.06      0.12       359\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.96      0.53      0.55      7809\n",
      "weighted avg       0.96      0.96      0.94      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_aid:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      6790\n",
      "           1       0.59      0.04      0.08      1019\n",
      "\n",
      "    accuracy                           0.87      7809\n",
      "   macro avg       0.73      0.52      0.50      7809\n",
      "weighted avg       0.84      0.87      0.82      7809\n",
      "\n",
      "\u001b[1mother_aid:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93      6790\n",
      "           1       0.66      0.04      0.08      1019\n",
      "\n",
      "    accuracy                           0.87      7809\n",
      "   macro avg       0.77      0.52      0.51      7809\n",
      "weighted avg       0.85      0.87      0.82      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1minfrastructure_related:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      7298\n",
      "           1       0.30      0.01      0.01       511\n",
      "\n",
      "    accuracy                           0.93      7809\n",
      "   macro avg       0.62      0.50      0.49      7809\n",
      "weighted avg       0.89      0.93      0.90      7809\n",
      "\n",
      "\u001b[1minfrastructure_related:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      7298\n",
      "           1       0.00      0.00      0.00       511\n",
      "\n",
      "    accuracy                           0.93      7809\n",
      "   macro avg       0.47      0.50      0.48      7809\n",
      "weighted avg       0.87      0.93      0.90      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mtransport:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7462\n",
      "           1       0.71      0.12      0.20       347\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.83      0.56      0.59      7809\n",
      "weighted avg       0.95      0.96      0.94      7809\n",
      "\n",
      "\u001b[1mtransport:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7462\n",
      "           1       0.57      0.01      0.02       347\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.76      0.51      0.50      7809\n",
      "weighted avg       0.94      0.96      0.93      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mbuildings:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7378\n",
      "           1       0.84      0.11      0.20       431\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.90      0.56      0.59      7809\n",
      "weighted avg       0.94      0.95      0.93      7809\n",
      "\n",
      "\u001b[1mbuildings:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7378\n",
      "           1       0.78      0.03      0.06       431\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.86      0.52      0.52      7809\n",
      "weighted avg       0.94      0.95      0.92      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1melectricity:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7658\n",
      "           1       1.00      0.03      0.06       151\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.99      0.52      0.53      7809\n",
      "weighted avg       0.98      0.98      0.97      7809\n",
      "\n",
      "\u001b[1melectricity:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7658\n",
      "           1       0.00      0.00      0.00       151\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.96      0.98      0.97      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mtools:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7757\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.50      0.50      0.50      7809\n",
      "weighted avg       0.99      0.99      0.99      7809\n",
      "\n",
      "\u001b[1mtools:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      7757\n",
      "           1       0.00      0.00      0.00        52\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.50      0.50      0.50      7809\n",
      "weighted avg       0.99      0.99      0.99      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mhospitals:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7725\n",
      "           1       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "\u001b[1mhospitals:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7725\n",
      "           1       0.00      0.00      0.00        84\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mshops:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7775\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           1.00      7809\n",
      "   macro avg       0.50      0.50      0.50      7809\n",
      "weighted avg       0.99      1.00      0.99      7809\n",
      "\n",
      "\u001b[1mshops:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7775\n",
      "           1       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           1.00      7809\n",
      "   macro avg       0.50      0.50      0.50      7809\n",
      "weighted avg       0.99      1.00      0.99      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1maid_centers:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7720\n",
      "           1       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "\u001b[1maid_centers:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7720\n",
      "           1       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_infrastructure:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7467\n",
      "           1       0.14      0.00      0.01       342\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.55      0.50      0.49      7809\n",
      "weighted avg       0.92      0.96      0.93      7809\n",
      "\n",
      "\u001b[1mother_infrastructure:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      7467\n",
      "           1       0.00      0.00      0.00       342\n",
      "\n",
      "    accuracy                           0.96      7809\n",
      "   macro avg       0.48      0.50      0.49      7809\n",
      "weighted avg       0.91      0.96      0.93      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mweather_related:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      5606\n",
      "           1       0.86      0.70      0.77      2203\n",
      "\n",
      "    accuracy                           0.88      7809\n",
      "   macro avg       0.88      0.83      0.85      7809\n",
      "weighted avg       0.88      0.88      0.88      7809\n",
      "\n",
      "\u001b[1mweather_related:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      5606\n",
      "           1       0.88      0.56      0.69      2203\n",
      "\n",
      "    accuracy                           0.86      7809\n",
      "   macro avg       0.86      0.77      0.80      7809\n",
      "weighted avg       0.86      0.86      0.84      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfloods:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7188\n",
      "           1       0.91      0.44      0.60       621\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.93      0.72      0.79      7809\n",
      "weighted avg       0.95      0.95      0.94      7809\n",
      "\n",
      "\u001b[1mfloods:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97      7188\n",
      "           1       0.91      0.25      0.40       621\n",
      "\n",
      "    accuracy                           0.94      7809\n",
      "   macro avg       0.93      0.63      0.68      7809\n",
      "weighted avg       0.94      0.94      0.92      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mstorm:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97      7067\n",
      "           1       0.79      0.50      0.61       742\n",
      "\n",
      "    accuracy                           0.94      7809\n",
      "   macro avg       0.87      0.74      0.79      7809\n",
      "weighted avg       0.93      0.94      0.93      7809\n",
      "\n",
      "\u001b[1mstorm:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96      7067\n",
      "           1       0.77      0.28      0.41       742\n",
      "\n",
      "    accuracy                           0.92      7809\n",
      "   macro avg       0.85      0.64      0.69      7809\n",
      "weighted avg       0.91      0.92      0.91      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfire:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7721\n",
      "           1       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "\u001b[1mfire:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      7721\n",
      "           1       0.00      0.00      0.00        88\n",
      "\n",
      "    accuracy                           0.99      7809\n",
      "   macro avg       0.49      0.50      0.50      7809\n",
      "weighted avg       0.98      0.99      0.98      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mearthquake:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      7067\n",
      "           1       0.90      0.80      0.85       742\n",
      "\n",
      "    accuracy                           0.97      7809\n",
      "   macro avg       0.94      0.90      0.92      7809\n",
      "weighted avg       0.97      0.97      0.97      7809\n",
      "\n",
      "\u001b[1mearthquake:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98      7067\n",
      "           1       0.90      0.59      0.71       742\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.93      0.79      0.84      7809\n",
      "weighted avg       0.95      0.95      0.95      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mcold:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7635\n",
      "           1       1.00      0.07      0.13       174\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.99      0.53      0.56      7809\n",
      "weighted avg       0.98      0.98      0.97      7809\n",
      "\n",
      "\u001b[1mcold:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      7635\n",
      "           1       1.00      0.01      0.02       174\n",
      "\n",
      "    accuracy                           0.98      7809\n",
      "   macro avg       0.99      0.51      0.51      7809\n",
      "weighted avg       0.98      0.98      0.97      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_weather:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7411\n",
      "           1       0.67      0.02      0.03       398\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.81      0.51      0.50      7809\n",
      "weighted avg       0.94      0.95      0.93      7809\n",
      "\n",
      "\u001b[1mother_weather:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97      7411\n",
      "           1       0.71      0.01      0.02       398\n",
      "\n",
      "    accuracy                           0.95      7809\n",
      "   macro avg       0.83      0.51      0.50      7809\n",
      "weighted avg       0.94      0.95      0.93      7809\n",
      "\n",
      "------------------------------------------------------------\n",
      "\u001b[1mdirect_report:\u001b[0m Pipeline 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91      6257\n",
      "           1       0.78      0.37      0.50      1552\n",
      "\n",
      "    accuracy                           0.85      7809\n",
      "   macro avg       0.82      0.67      0.71      7809\n",
      "weighted avg       0.85      0.85      0.83      7809\n",
      "\n",
      "\u001b[1mdirect_report:\u001b[0m Pipeline 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.91      6257\n",
      "           1       0.79      0.36      0.49      1552\n",
      "\n",
      "    accuracy                           0.85      7809\n",
      "   macro avg       0.83      0.67      0.70      7809\n",
      "weighted avg       0.85      0.85      0.83      7809\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print classification reports\n",
    "for i, col in enumerate(Y_test.columns):\n",
    "    print(f'\\033[1m{col}:\\033[0m Pipeline 1')\n",
    "    print(classification_report(Y_test[col], Y_pred_multi[:, i], zero_division=0))\n",
    "    print(f'\\033[1m{col}:\\033[0m Pipeline 2')\n",
    "    print(classification_report(Y_test[col], Y_pred_single[:, i], zero_division=0))\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I set `zero_division=0`; this means when there are no predicted instances in a class, the precision is set to zero (there is a division by zero in its calculation, since there are no positive predictions). The precision is not meaningful in this case, so I may as well just set it to zero instead of using the default `zero_division='warn'`, which also sets it to zero but has the annoying effect of printing a bunch of warnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers look fairly similar, though the first pipeline consistently does a bit better. I'll check this a bit more explicitly as follows. What I'm really concerned with are the F1 score (harmonic mean of precision and recall) and the recall of the predictors on the '1' class, that is, the class of messages that are known to be in the categories after which they are named (for example, a message in the '1' class in 'medical_help' indicates that medical help is needed). \n",
    "\n",
    "In fact, I care signficantly more about recall than precision for this class: For each column with the exception of 'related', the vast majority of messages are in the '0' class, meaning not in that category. The goal is to pick out (recall) from this vast sea of messages almost all the messages in class '1' and forward those to human disaster response professionals, who can then quickly determine whether the message is relevant. Of course, I could have a recall of 1 in this class by just having the predictor predict class '1' on every message, but that would be pointless - the professionals would then have a ton of work to do to sift through all those messages. It depends on the size of the class imbalance, but typically a precision of 50% is fine - even though half the predicted '1's are false positives in this case, that's still a less work for the humans.\n",
    "\n",
    "What does this mean? F1 score is certainly better to look at than raw accuracy (not a good metric at all due to class imbalance), but it's not ideal for most targets since the class imbalance is generally quite large. So here I'll look at the F1 score and recall explicitly between the two predictors; later I will probably create custom metrics that treat the precision/recall tradeoff in a more sensible way for each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mrelated:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.891 vs 0.895\n",
      "recall: 0.953 vs 0.925\n",
      "------------------------------------------------------------\n",
      "\u001b[1mrequest:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.62 vs 0.62\n",
      "recall: 0.503 vs 0.506\n",
      "------------------------------------------------------------\n",
      "\u001b[1moffer:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0 vs 0\n",
      "recall: 0 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1maid_related:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.739 vs 0.656\n",
      "recall: 0.713 vs 0.553\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmedical_help:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.122 vs 0.0345\n",
      "recall: 0.0663 vs 0.0178\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmedical_products:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.13 vs 0.0585\n",
      "recall: 0.0709 vs 0.0304\n",
      "------------------------------------------------------------\n",
      "\u001b[1msearch_and_rescue:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0776 vs 0.0439\n",
      "recall: 0.0407 vs 0.0226\n",
      "------------------------------------------------------------\n",
      "\u001b[1msecurity:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0126 vs 0.0126\n",
      "recall: 0.00633 vs 0.00633\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmilitary:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.118 vs 0.0458\n",
      "recall: 0.0643 vs 0.0241\n",
      "------------------------------------------------------------\n",
      "\u001b[1mwater:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.535 vs 0.397\n",
      "recall: 0.379 vs 0.253\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfood:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.704 vs 0.613\n",
      "recall: 0.615 vs 0.479\n",
      "------------------------------------------------------------\n",
      "\u001b[1mshelter:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.526 vs 0.378\n",
      "recall: 0.388 vs 0.244\n",
      "------------------------------------------------------------\n",
      "\u001b[1mclothing:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.174 vs 0.175\n",
      "recall: 0.0976 vs 0.0976\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmoney:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0909 vs 0.0812\n",
      "recall: 0.0481 vs 0.0428\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmissing_people:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.025 vs 0\n",
      "recall: 0.0127 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mrefugees:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0469 vs 0.0234\n",
      "recall: 0.0242 vs 0.0121\n",
      "------------------------------------------------------------\n",
      "\u001b[1mdeath:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.299 vs 0.115\n",
      "recall: 0.181 vs 0.0613\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_aid:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0754 vs 0.081\n",
      "recall: 0.0402 vs 0.0432\n",
      "------------------------------------------------------------\n",
      "\u001b[1minfrastructure_related:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0115 vs 0\n",
      "recall: 0.00587 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mtransport:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.202 vs 0.0226\n",
      "recall: 0.118 vs 0.0115\n",
      "------------------------------------------------------------\n",
      "\u001b[1mbuildings:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.197 vs 0.0624\n",
      "recall: 0.111 vs 0.0325\n",
      "------------------------------------------------------------\n",
      "\u001b[1melectricity:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0641 vs 0\n",
      "recall: 0.0331 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mtools:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0 vs 0\n",
      "recall: 0 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mhospitals:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0 vs 0\n",
      "recall: 0 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mshops:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0 vs 0\n",
      "recall: 0 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1maid_centers:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0 vs 0\n",
      "recall: 0 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_infrastructure:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.00573 vs 0\n",
      "recall: 0.00292 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mweather_related:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.774 vs 0.688\n",
      "recall: 0.704 vs 0.565\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfloods:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.597 vs 0.396\n",
      "recall: 0.443 vs 0.253\n",
      "------------------------------------------------------------\n",
      "\u001b[1mstorm:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.615 vs 0.415\n",
      "recall: 0.501 vs 0.283\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfire:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0 vs 0\n",
      "recall: 0 vs 0\n",
      "------------------------------------------------------------\n",
      "\u001b[1mearthquake:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.847 vs 0.711\n",
      "recall: 0.803 vs 0.589\n",
      "------------------------------------------------------------\n",
      "\u001b[1mcold:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.129 vs 0.0227\n",
      "recall: 0.069 vs 0.0115\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_weather:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.0295 vs 0.0247\n",
      "recall: 0.0151 vs 0.0126\n",
      "------------------------------------------------------------\n",
      "\u001b[1mdirect_report:\u001b[0m Pipeline 1 vs 2\n",
      "F1 score: 0.499 vs 0.494\n",
      "recall: 0.367 vs 0.359\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show the F1 score and recall for each pipeline, for each target\n",
    "for i, col in enumerate(Y_test.columns):\n",
    "    print(f'\\033[1m{col}:\\033[0m Pipeline 1 vs 2')\n",
    "    report_1 = classification_report(\n",
    "        Y_test[col], Y_pred_multi[:, i], zero_division=0, output_dict=True)\n",
    "    report_2 = classification_report(\n",
    "        Y_test[col], Y_pred_single[:, i], zero_division=0, output_dict=True)\n",
    "    f1_score_1 = report_1['1']['f1-score']\n",
    "    f1_score_2 = report_2['1']['f1-score']\n",
    "    recall_1 = report_1['1']['recall']\n",
    "    recall_2 = report_2['1']['recall']\n",
    "    print(f'F1 score: {f1_score_1:.3g} vs {f1_score_2:.3g}')\n",
    "    print(f'recall: {recall_1:.3g} vs {recall_2:.3g}')\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the first pipeline performs a bit better than the second, which is not surprising since the first fits independent predictors for each target. However, training using the first pipeline is quite slow, and on my laptop it is rather impractical for hyperparameter optimization. Therefore I'll go with pipeline 2 for now, and may later come back to pipeline 1 once I have a better idea of the right hyperparameters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to find better hyperparameters using cross-validation and grid search. Sklearn's `GridSearchCV()` method is convenient for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I take a look at the model's hyperparameter names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(stop_words=['of', 'a', 'aren', 'during', 'isn', 'above', 'more',\n",
       "                               'to', 'too', 'both', 'mightn', 'him', 'hers', 'our',\n",
       "                               'doesn', 'at', 'we', 'before', 'your', 'doing',\n",
       "                               'ourselves', 'ain', 's', 'do', 'ours', 'up',\n",
       "                               'against', 'such', 'i', 'my', ...],\n",
       "                   tokenizer=<function tokenize at 0x12eb58940>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', RandomForestClassifier(random_state=42))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(stop_words=['of', 'a', 'aren', 'during', 'isn', 'above', 'more',\n",
       "                             'to', 'too', 'both', 'mightn', 'him', 'hers', 'our',\n",
       "                             'doesn', 'at', 'we', 'before', 'your', 'doing',\n",
       "                             'ourselves', 'ain', 's', 'do', 'ours', 'up',\n",
       "                             'against', 'such', 'i', 'my', ...],\n",
       "                 tokenizer=<function tokenize at 0x12eb58940>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': RandomForestClassifier(random_state=42),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': ['of',\n",
       "  'a',\n",
       "  'aren',\n",
       "  'during',\n",
       "  'isn',\n",
       "  'above',\n",
       "  'more',\n",
       "  'to',\n",
       "  'too',\n",
       "  'both',\n",
       "  'mightn',\n",
       "  'him',\n",
       "  'hers',\n",
       "  'our',\n",
       "  'doesn',\n",
       "  'at',\n",
       "  'we',\n",
       "  'before',\n",
       "  'your',\n",
       "  'doing',\n",
       "  'ourselves',\n",
       "  'ain',\n",
       "  's',\n",
       "  'do',\n",
       "  'ours',\n",
       "  'up',\n",
       "  'against',\n",
       "  'such',\n",
       "  'i',\n",
       "  'my',\n",
       "  'yourself',\n",
       "  'yours',\n",
       "  'they',\n",
       "  'and',\n",
       "  'over',\n",
       "  'no',\n",
       "  'being',\n",
       "  'having',\n",
       "  'haven',\n",
       "  'he',\n",
       "  'when',\n",
       "  'couldn',\n",
       "  'weren',\n",
       "  'down',\n",
       "  'very',\n",
       "  'doe',\n",
       "  'if',\n",
       "  'm',\n",
       "  'shan',\n",
       "  'don',\n",
       "  'now',\n",
       "  'wa',\n",
       "  'not',\n",
       "  'just',\n",
       "  'you',\n",
       "  'why',\n",
       "  'me',\n",
       "  'than',\n",
       "  've',\n",
       "  'are',\n",
       "  'between',\n",
       "  'into',\n",
       "  'wasn',\n",
       "  'where',\n",
       "  'other',\n",
       "  'his',\n",
       "  'under',\n",
       "  'because',\n",
       "  'd',\n",
       "  'again',\n",
       "  'from',\n",
       "  'by',\n",
       "  'were',\n",
       "  'any',\n",
       "  'until',\n",
       "  'is',\n",
       "  'about',\n",
       "  'further',\n",
       "  'what',\n",
       "  'them',\n",
       "  'be',\n",
       "  'most',\n",
       "  'theirs',\n",
       "  'needn',\n",
       "  'who',\n",
       "  'each',\n",
       "  'after',\n",
       "  'yourselves',\n",
       "  'with',\n",
       "  'am',\n",
       "  'those',\n",
       "  'same',\n",
       "  'out',\n",
       "  'all',\n",
       "  'on',\n",
       "  'hadn',\n",
       "  'shouldn',\n",
       "  'for',\n",
       "  're',\n",
       "  'which',\n",
       "  'the',\n",
       "  'own',\n",
       "  'been',\n",
       "  'while',\n",
       "  'himself',\n",
       "  'themselves',\n",
       "  'had',\n",
       "  'through',\n",
       "  'it',\n",
       "  'some',\n",
       "  'in',\n",
       "  'hasn',\n",
       "  'an',\n",
       "  'here',\n",
       "  'so',\n",
       "  'll',\n",
       "  'herself',\n",
       "  'won',\n",
       "  'below',\n",
       "  'whom',\n",
       "  'how',\n",
       "  'but',\n",
       "  'her',\n",
       "  'these',\n",
       "  'few',\n",
       "  'their',\n",
       "  'off',\n",
       "  'or',\n",
       "  'then',\n",
       "  'y',\n",
       "  'once',\n",
       "  'ha',\n",
       "  'there',\n",
       "  'she',\n",
       "  'did',\n",
       "  'itself',\n",
       "  'have',\n",
       "  'only',\n",
       "  'should',\n",
       "  'o',\n",
       "  'didn',\n",
       "  'ma',\n",
       "  'this',\n",
       "  'can',\n",
       "  'that',\n",
       "  'will',\n",
       "  't',\n",
       "  'wouldn',\n",
       "  'mustn',\n",
       "  'nor',\n",
       "  'myself'],\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__ccp_alpha': 0.0,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__max_samples': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 100,\n",
       " 'clf__n_jobs': None,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': 42,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the hyperparameter names\n",
    "pipe_single.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll write some functions to do grid search in a convenient way and print metrics for evaluating predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(Y_true, Y_pred):\n",
    "    '''Takes predicted and true labels and prints\n",
    "        the accuracy, precision, recall, and F1 score.\n",
    "    Args:\n",
    "        Y_true -- an array or dataframe of the true class labels.\n",
    "        Y_pred -- an array or dataframe of the predicted class labels.\n",
    "    Returns:\n",
    "        None; instead, prints the accuracy, precision, recall, and \n",
    "        F1 score.\n",
    "    '''\n",
    "    eq_arr = np.array(Y_pred == Y_true)\n",
    "    pos_arr = np.array(Y_true == 1)\n",
    "    neg_arr = np.array(Y_true == 0)\n",
    "    P = np.sum(pos_arr)\n",
    "    TP = np.sum(pos_arr*eq_arr)\n",
    "    FN = P - TP\n",
    "    N = np.sum(neg_arr)\n",
    "    TN = np.sum(neg_arr*eq_arr)\n",
    "    FP = N - TN\n",
    "    accuracy = (TP + TN)/(P + N)\n",
    "    precision = TP/(TP + FP)\n",
    "    recall = TP/P\n",
    "    f1 = 2/(1/precision + 1/recall)\n",
    "    print(f'accuracy = {accuracy:.3g}')\n",
    "    print(f'precision = {precision:.3g}')\n",
    "    print(f'recall = {recall:.3g}')\n",
    "    print(f'F1 score = {f1:.3g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grid_search(pipe, parameters, X_train, Y_train, scorer=None, n_jobs=1, cv=None):\n",
    "    '''Does a grid search to optimize hyperparameters.\n",
    "    Args:\n",
    "        pipe -- a Pipe() object representing a machine learning pipeline.\n",
    "        parameters -- a dict of parameters to be used by GridSearchCV().\n",
    "        Y_true -- a dataframe or array of the true class labels.\n",
    "        scorer -- an optional scoring function.\n",
    "        n_jobs -- the number of jobs to run in parallel. The default is 1, \n",
    "                    though I will typically use 7 since my Macbook Air has\n",
    "                    8 CPU cores.\n",
    "        cv -- something to be used for cross validation, such as an instance\n",
    "                of StratifiedShuffleSplit().\n",
    "    Returns:\n",
    "        result_cv -- the object output by GridSearchCV() after fitting, with\n",
    "                all its parameters and methods accessible.\n",
    "        Also displays the dataframe of CV results and prints the\n",
    "        total runtime for training and testing.\n",
    "    '''\n",
    "    # Create the GridSearchCV object with default 5-fold cross-validation\n",
    "    result_cv = GridSearchCV(pipe_single, parameters, refit=True, scoring=scorer,\n",
    "                             n_jobs=n_jobs, cv=cv)\n",
    "    # Do fits and find the runtime\n",
    "    t0 = perf_counter()\n",
    "    result_cv.fit(X_train, Y_train)\n",
    "    # Display the results\n",
    "    display(pd.DataFrame(result_cv.cv_results_))\n",
    "    t1 = perf_counter()\n",
    "    print(f'Runtime = {t1 - t0: .3g} seconds')\n",
    "    return result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if this is working properly, I first try it with just one parameter value (which is the same as in the fit in the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.785512</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>2.472597</td>\n",
       "      <td>0.015862</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__max_features': None}</td>\n",
       "      <td>0.288968</td>\n",
       "      <td>0.301043</td>\n",
       "      <td>0.29034</td>\n",
       "      <td>0.295635</td>\n",
       "      <td>0.291243</td>\n",
       "      <td>0.293446</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      39.785512      0.018044         2.472597        0.015862   \n",
       "\n",
       "  param_vect__max_features                        params  split0_test_score  \\\n",
       "0                     None  {'vect__max_features': None}           0.288968   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.301043            0.29034           0.295635           0.291243   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.293446        0.004406                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  85.6 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vect',\n",
       "                                        CountVectorizer(stop_words=['of', 'a',\n",
       "                                                                    'aren',\n",
       "                                                                    'during',\n",
       "                                                                    'isn',\n",
       "                                                                    'above',\n",
       "                                                                    'more',\n",
       "                                                                    'to', 'too',\n",
       "                                                                    'both',\n",
       "                                                                    'mightn',\n",
       "                                                                    'him',\n",
       "                                                                    'hers',\n",
       "                                                                    'our',\n",
       "                                                                    'doesn',\n",
       "                                                                    'at', 'we',\n",
       "                                                                    'before',\n",
       "                                                                    'your',\n",
       "                                                                    'doing',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'ain', 's',\n",
       "                                                                    'do',\n",
       "                                                                    'ours',\n",
       "                                                                    'up',\n",
       "                                                                    'against',\n",
       "                                                                    'such', 'i',\n",
       "                                                                    'my', ...],\n",
       "                                                        tokenizer=<function tokenize at 0x12eb58940>)),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf',\n",
       "                                        RandomForestClassifier(random_state=42))]),\n",
       "             n_jobs=7, param_grid={'vect__max_features': [None]})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'vect__max_features': [None]\n",
    "              }\n",
    "\n",
    "do_grid_search(pipe_single, parameters, X_train, Y_train, n_jobs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, I'm not sure how the test scores are calculated. I will create a custom scoring function instead of relying on the default behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(Y_true, Y_pred, c=1, weights='equal'):\n",
    "    '''Takes the true and predicted class labels and returns\n",
    "        a score that is similar to the F1 score, but weighted\n",
    "        to favor the recall by a factor of c.\n",
    "        Using the default c=1 causes the F1 score to be returned.\n",
    "        Using c='recall' causes the recall to be returned.\n",
    "    Args:\n",
    "        Y_true -- an array or dataframe of true class labels.\n",
    "        Y_pred -- an array or dataframe of predicted class labels.\n",
    "        c -- a nonzero number that weights the recall in the score \n",
    "             for a column,\n",
    "             or the string 'recall', meaning the recall is the score\n",
    "             for each column.\n",
    "        weights -- a numeric list of length Y_true.shape[1] that gives \n",
    "                   the weight assigned to each target (column) in the\n",
    "                   final score. It can also be the string 'equal', \n",
    "                   which gives equal weights of size 1/Y_true.shape[1].\n",
    "    Returns:\n",
    "        score -- a score for the predictions over all targets, \n",
    "                 weighted for each target by some choice of weights.\n",
    "    '''\n",
    "    score_list = []\n",
    "    Y_true = np.array(Y_true)\n",
    "    Y_pred = np.array(Y_pred)\n",
    "    if Y_true.ndim == 1:\n",
    "        Y_true = np.reshape(Y_true, (-1, 1))\n",
    "    if Y_pred.ndim == 1:\n",
    "        Y_pred = np.reshape(Y_pred, (-1, 1))\n",
    "    for col_number in range(Y_true.shape[1]):\n",
    "        pred = Y_pred[:, col_number]\n",
    "        true = Y_true[:, col_number]\n",
    "        eq_arr = np.array(pred == true)\n",
    "        pos_arr = np.array(true == 1)\n",
    "        neg_arr = np.array(true == 0)\n",
    "        P = np.sum(pos_arr)\n",
    "        TP = np.sum(pos_arr*eq_arr)\n",
    "        FN = P - TP\n",
    "        N = np.sum(neg_arr)\n",
    "        TN = np.sum(neg_arr*eq_arr)\n",
    "        FP = N - TN\n",
    "        accuracy = (TP + TN)/(P + N)\n",
    "        # Note I need to deal with the case where the calculation of \n",
    "        # precision involves dividing by zero; in this case I will\n",
    "        # set the precision to be zero.\n",
    "        precision = 0 if (TP + FP == 0) else TP/(TP + FP)\n",
    "        recall = TP/P\n",
    "        if c == 'recall':\n",
    "            score_list.append(recall)\n",
    "        else:\n",
    "            if recall*precision == 0:\n",
    "                # If either of recall or precision is zero,\n",
    "                # then I set the score to be zero.\n",
    "                score_list.append(0)\n",
    "            else:\n",
    "                score_list.append(2/(1/precision + 1/(c*recall)))\n",
    "    score_arr = np.array(score_list)\n",
    "    if weights == 'equal':\n",
    "        weights = (1/Y_true.shape[1])*np.ones(Y_true.shape[1])\n",
    "    score = sum(score_arr*weights)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pipe_multi.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2615856066781391"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scorer using sklearn's `make_scorer` function\n",
    "# Here c = 1, so this is the F1 score\n",
    "my_scorer = make_scorer(score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.427753</td>\n",
       "      <td>0.537777</td>\n",
       "      <td>1.971462</td>\n",
       "      <td>0.195831</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__max_features': None}</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>0.198295</td>\n",
       "      <td>0.203314</td>\n",
       "      <td>0.196648</td>\n",
       "      <td>0.20256</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      40.427753      0.537777         1.971462        0.195831   \n",
       "\n",
       "  param_vect__max_features                        params  split0_test_score  \\\n",
       "0                     None  {'vect__max_features': None}           0.204039   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.210502           0.198295           0.203314           0.196648   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0          0.20256        0.004878                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  84.7 seconds\n"
     ]
    }
   ],
   "source": [
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train, scorer=my_scorer, n_jobs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this makes sense to me: I've set things up so that the hyperparameter values in the `parameters` grid that maximize the F1 score (on instances labeled 1) are chosen. Now I will do a real hyperparameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I may want to limit the number of features produced by the vectorizer (i.e. cut off some of the least common words). First, I'll check how many features it produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25615"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of features produced by the vectorizer\n",
    "len(pipe_single['vect'].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll try some smaller values (which will result in shorter fitting times) and see if that produces better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also try both $L^1$ and $L^2$ norms for the TF.IDF transformer, as well as several values for the number of estimators, minimum samples split, and class weight strategy for the random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtimes here are still fairly long, at least for a personal project on my laptop, so I'll do things somewhat iteratively instead of just running a full grid search. This may not be as good as a full grid search, since the parameters are not necessarily independent of each other. For example, I could search just over the number of features from the count vectorizer and find that value $n$ is the best (with the other hyperparameter values fixed). Then I could search over the number of estimators and find some optimal value $m$ there. But there's no guarantee that using the values $n$ and $m$ for these respective parameters will be the best combination in the two-dimensional space of those parameters. However, I've found in my personal projects that an iterative approach often works reasonably well, and often the perfect is the enemy of the good since the runtimes are so long for a full grid search. Ultimately, I am looking to make my model better, and will be satisfied to some extent by improvement even if there is a better set of hyperpameters out there somewhere that would take much longer to find.\n",
    "\n",
    "Now, to get started. First, I want to get an idea of how many features from the count vectorizer to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.300950</td>\n",
       "      <td>0.540791</td>\n",
       "      <td>2.660276</td>\n",
       "      <td>0.127377</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__max_features': None}</td>\n",
       "      <td>0.204039</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>0.198295</td>\n",
       "      <td>0.203314</td>\n",
       "      <td>0.196648</td>\n",
       "      <td>0.202560</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.558283</td>\n",
       "      <td>0.822314</td>\n",
       "      <td>2.399382</td>\n",
       "      <td>0.105517</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'vect__max_features': 10000}</td>\n",
       "      <td>0.215233</td>\n",
       "      <td>0.217657</td>\n",
       "      <td>0.211693</td>\n",
       "      <td>0.216754</td>\n",
       "      <td>0.207432</td>\n",
       "      <td>0.213754</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.065170</td>\n",
       "      <td>0.181855</td>\n",
       "      <td>2.423074</td>\n",
       "      <td>0.086247</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'vect__max_features': 5000}</td>\n",
       "      <td>0.233381</td>\n",
       "      <td>0.229144</td>\n",
       "      <td>0.222975</td>\n",
       "      <td>0.227927</td>\n",
       "      <td>0.221848</td>\n",
       "      <td>0.227055</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.143414</td>\n",
       "      <td>0.121270</td>\n",
       "      <td>2.233253</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>2500</td>\n",
       "      <td>{'vect__max_features': 2500}</td>\n",
       "      <td>0.237244</td>\n",
       "      <td>0.243949</td>\n",
       "      <td>0.228733</td>\n",
       "      <td>0.238439</td>\n",
       "      <td>0.235735</td>\n",
       "      <td>0.236820</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.172210</td>\n",
       "      <td>3.182714</td>\n",
       "      <td>1.546184</td>\n",
       "      <td>0.265730</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'vect__max_features': 1000}</td>\n",
       "      <td>0.249451</td>\n",
       "      <td>0.257467</td>\n",
       "      <td>0.245812</td>\n",
       "      <td>0.248323</td>\n",
       "      <td>0.248546</td>\n",
       "      <td>0.249920</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      50.300950      0.540791         2.660276        0.127377   \n",
       "1      42.558283      0.822314         2.399382        0.105517   \n",
       "2      40.065170      0.181855         2.423074        0.086247   \n",
       "3      38.143414      0.121270         2.233253        0.065808   \n",
       "4      27.172210      3.182714         1.546184        0.265730   \n",
       "\n",
       "  param_vect__max_features                         params  split0_test_score  \\\n",
       "0                     None   {'vect__max_features': None}           0.204039   \n",
       "1                    10000  {'vect__max_features': 10000}           0.215233   \n",
       "2                     5000   {'vect__max_features': 5000}           0.233381   \n",
       "3                     2500   {'vect__max_features': 2500}           0.237244   \n",
       "4                     1000   {'vect__max_features': 1000}           0.249451   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.210502           0.198295           0.203314           0.196648   \n",
       "1           0.217657           0.211693           0.216754           0.207432   \n",
       "2           0.229144           0.222975           0.227927           0.221848   \n",
       "3           0.243949           0.228733           0.238439           0.235735   \n",
       "4           0.257467           0.245812           0.248323           0.248546   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.202560        0.004878                5  \n",
       "1         0.213754        0.003759                4  \n",
       "2         0.227055        0.004217                3  \n",
       "3         0.236820        0.004903                2  \n",
       "4         0.249920        0.003962                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  188 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [None, 10000, 5000, 2500, 1000], \n",
    "              }\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train,\n",
    "                    scorer=my_scorer, n_jobs=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Using just 1000 of these features works best, though the improvement in the F1 score is not huge. Unfortunately, it appears I didn't choose the lowest value to be small enough to find the maximum mean test score, so I'll try some more values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.115089</td>\n",
       "      <td>0.025233</td>\n",
       "      <td>2.922883</td>\n",
       "      <td>0.011660</td>\n",
       "      <td>900</td>\n",
       "      <td>{'vect__max_features': 900}</td>\n",
       "      <td>0.250949</td>\n",
       "      <td>0.256239</td>\n",
       "      <td>0.247496</td>\n",
       "      <td>0.250167</td>\n",
       "      <td>0.246433</td>\n",
       "      <td>0.250257</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.880285</td>\n",
       "      <td>0.641064</td>\n",
       "      <td>2.451122</td>\n",
       "      <td>0.315770</td>\n",
       "      <td>700</td>\n",
       "      <td>{'vect__max_features': 700}</td>\n",
       "      <td>0.258448</td>\n",
       "      <td>0.260738</td>\n",
       "      <td>0.247388</td>\n",
       "      <td>0.249220</td>\n",
       "      <td>0.246348</td>\n",
       "      <td>0.252428</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30.109074</td>\n",
       "      <td>0.806639</td>\n",
       "      <td>2.590239</td>\n",
       "      <td>0.317651</td>\n",
       "      <td>500</td>\n",
       "      <td>{'vect__max_features': 500}</td>\n",
       "      <td>0.259372</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>0.253090</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.257081</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.355609</td>\n",
       "      <td>0.035785</td>\n",
       "      <td>2.395975</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>300</td>\n",
       "      <td>{'vect__max_features': 300}</td>\n",
       "      <td>0.244194</td>\n",
       "      <td>0.252744</td>\n",
       "      <td>0.249532</td>\n",
       "      <td>0.251776</td>\n",
       "      <td>0.242762</td>\n",
       "      <td>0.248202</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      33.115089      0.025233         2.922883        0.011660   \n",
       "1      31.880285      0.641064         2.451122        0.315770   \n",
       "2      30.109074      0.806639         2.590239        0.317651   \n",
       "3      25.355609      0.035785         2.395975        0.009693   \n",
       "\n",
       "  param_vect__max_features                       params  split0_test_score  \\\n",
       "0                      900  {'vect__max_features': 900}           0.250949   \n",
       "1                      700  {'vect__max_features': 700}           0.258448   \n",
       "2                      500  {'vect__max_features': 500}           0.259372   \n",
       "3                      300  {'vect__max_features': 300}           0.244194   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.256239           0.247496           0.250167           0.246433   \n",
       "1           0.260738           0.247388           0.249220           0.246348   \n",
       "2           0.267971           0.255008           0.253090           0.249962   \n",
       "3           0.252744           0.249532           0.251776           0.242762   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.250257        0.003421                3  \n",
       "1         0.252428        0.005966                2  \n",
       "2         0.257081        0.006241                1  \n",
       "3         0.248202        0.004021                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  122 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [900, 700, 500, 300], \n",
    "              }\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train,\n",
    "                    scorer=my_scorer, n_jobs=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the standard errors (which are the standard deviations `std_test_score` divided by $\\sqrt{5})$, the mean test scores using 900, 700, and 500 features are statistically indistinguishable from each other, but using 300 features is clearly worse. I'll use 500 features to keep the runtimes down. Next, I'll try comparing using balanced class weights versus the default of equal class weights of 1, and I'll also see whether using the $L^1$ or $L^2$ norm for the TF.IDF transformer works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <th>param_tfidf__norm</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.077774</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>3.198230</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>None</td>\n",
       "      <td>l1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__class_weight': None, 'tfidf__norm': 'l1...</td>\n",
       "      <td>0.256261</td>\n",
       "      <td>0.259777</td>\n",
       "      <td>0.247116</td>\n",
       "      <td>0.249480</td>\n",
       "      <td>0.250273</td>\n",
       "      <td>0.252581</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.439278</td>\n",
       "      <td>0.324433</td>\n",
       "      <td>3.166538</td>\n",
       "      <td>0.027567</td>\n",
       "      <td>None</td>\n",
       "      <td>l2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__class_weight': None, 'tfidf__norm': 'l2...</td>\n",
       "      <td>0.259372</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>0.253090</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.257081</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.341249</td>\n",
       "      <td>1.228560</td>\n",
       "      <td>3.150687</td>\n",
       "      <td>0.674604</td>\n",
       "      <td>balanced</td>\n",
       "      <td>l1</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'tfidf__norm...</td>\n",
       "      <td>0.172416</td>\n",
       "      <td>0.179246</td>\n",
       "      <td>0.166122</td>\n",
       "      <td>0.168381</td>\n",
       "      <td>0.170931</td>\n",
       "      <td>0.171419</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.643835</td>\n",
       "      <td>0.371092</td>\n",
       "      <td>3.030084</td>\n",
       "      <td>0.208228</td>\n",
       "      <td>balanced</td>\n",
       "      <td>l2</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'tfidf__norm...</td>\n",
       "      <td>0.177995</td>\n",
       "      <td>0.184175</td>\n",
       "      <td>0.179835</td>\n",
       "      <td>0.177951</td>\n",
       "      <td>0.175977</td>\n",
       "      <td>0.179187</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      30.077774      0.013312         3.198230        0.010278   \n",
       "1      30.439278      0.324433         3.166538        0.027567   \n",
       "2      29.341249      1.228560         3.150687        0.674604   \n",
       "3      27.643835      0.371092         3.030084        0.208228   \n",
       "\n",
       "  param_clf__class_weight param_tfidf__norm param_vect__max_features  \\\n",
       "0                    None                l1                      500   \n",
       "1                    None                l2                      500   \n",
       "2                balanced                l1                      500   \n",
       "3                balanced                l2                      500   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf__class_weight': None, 'tfidf__norm': 'l1...           0.256261   \n",
       "1  {'clf__class_weight': None, 'tfidf__norm': 'l2...           0.259372   \n",
       "2  {'clf__class_weight': 'balanced', 'tfidf__norm...           0.172416   \n",
       "3  {'clf__class_weight': 'balanced', 'tfidf__norm...           0.177995   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.259777           0.247116           0.249480           0.250273   \n",
       "1           0.267971           0.255008           0.253090           0.249962   \n",
       "2           0.179246           0.166122           0.168381           0.170931   \n",
       "3           0.184175           0.179835           0.177951           0.175977   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.252581        0.004693                2  \n",
       "1         0.257081        0.006241                1  \n",
       "2         0.171419        0.004467                4  \n",
       "3         0.179187        0.002777                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  122 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [500],\n",
    "              'tfidf__norm': ['l1', 'l2'], \n",
    "              'clf__class_weight': [None, 'balanced']\n",
    "              }\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train, \n",
    "                    scorer=my_scorer, n_jobs=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using balanced class weights clearly does worse than not using them. There isn't much difference in performance between $L^1$ and $L^2$ metrics being used, with $L^2$ being just slightly better.\n",
    "\n",
    "It's interesting that using balanced class weights in `RandomForestClassifier` doesn't yield better results; for most of the target columns, the classes are quite imbalanced, so I would expect using balanced class weights to help. I suspect that I need to train different random forests for each of the target columns to take advantage of the balanced weights approach, as the imbalance varies a lot between the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll try a few values for the minimum number of samples required to split an internal node (`min_samples_split`) and the number of trees in the forest (`n_estimators`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__min_samples_split</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.736962</td>\n",
       "      <td>0.038441</td>\n",
       "      <td>1.431813</td>\n",
       "      <td>0.056901</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.255531</td>\n",
       "      <td>0.261703</td>\n",
       "      <td>0.256105</td>\n",
       "      <td>0.251323</td>\n",
       "      <td>0.252570</td>\n",
       "      <td>0.255446</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.038532</td>\n",
       "      <td>0.744772</td>\n",
       "      <td>2.418251</td>\n",
       "      <td>0.203768</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.259372</td>\n",
       "      <td>0.267971</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>0.253090</td>\n",
       "      <td>0.249962</td>\n",
       "      <td>0.257081</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.077842</td>\n",
       "      <td>0.449682</td>\n",
       "      <td>4.579705</td>\n",
       "      <td>0.292684</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.262370</td>\n",
       "      <td>0.268882</td>\n",
       "      <td>0.254261</td>\n",
       "      <td>0.256976</td>\n",
       "      <td>0.254217</td>\n",
       "      <td>0.259341</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.736596</td>\n",
       "      <td>0.178985</td>\n",
       "      <td>1.474098</td>\n",
       "      <td>0.033142</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.260934</td>\n",
       "      <td>0.267176</td>\n",
       "      <td>0.255395</td>\n",
       "      <td>0.252899</td>\n",
       "      <td>0.256447</td>\n",
       "      <td>0.258570</td>\n",
       "      <td>0.005028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.215677</td>\n",
       "      <td>0.265763</td>\n",
       "      <td>2.209244</td>\n",
       "      <td>0.291816</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.257970</td>\n",
       "      <td>0.266255</td>\n",
       "      <td>0.255736</td>\n",
       "      <td>0.251952</td>\n",
       "      <td>0.252136</td>\n",
       "      <td>0.256810</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.021841</td>\n",
       "      <td>0.103245</td>\n",
       "      <td>4.125410</td>\n",
       "      <td>0.390502</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.260316</td>\n",
       "      <td>0.267578</td>\n",
       "      <td>0.254942</td>\n",
       "      <td>0.254343</td>\n",
       "      <td>0.252486</td>\n",
       "      <td>0.257933</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16.633111</td>\n",
       "      <td>0.110714</td>\n",
       "      <td>1.470584</td>\n",
       "      <td>0.113113</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.258780</td>\n",
       "      <td>0.266520</td>\n",
       "      <td>0.257201</td>\n",
       "      <td>0.253050</td>\n",
       "      <td>0.251246</td>\n",
       "      <td>0.257359</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.267295</td>\n",
       "      <td>0.157156</td>\n",
       "      <td>2.030930</td>\n",
       "      <td>0.097063</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.254766</td>\n",
       "      <td>0.264434</td>\n",
       "      <td>0.256616</td>\n",
       "      <td>0.251844</td>\n",
       "      <td>0.248266</td>\n",
       "      <td>0.255185</td>\n",
       "      <td>0.005419</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47.068951</td>\n",
       "      <td>3.348214</td>\n",
       "      <td>2.786811</td>\n",
       "      <td>0.595060</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>500</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.257434</td>\n",
       "      <td>0.263399</td>\n",
       "      <td>0.252524</td>\n",
       "      <td>0.251068</td>\n",
       "      <td>0.247593</td>\n",
       "      <td>0.254404</td>\n",
       "      <td>0.005498</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      16.736962      0.038441         1.431813        0.056901   \n",
       "1      31.038532      0.744772         2.418251        0.203768   \n",
       "2      60.077842      0.449682         4.579705        0.292684   \n",
       "3      16.736596      0.178985         1.474098        0.033142   \n",
       "4      30.215677      0.265763         2.209244        0.291816   \n",
       "5      57.021841      0.103245         4.125410        0.390502   \n",
       "6      16.633111      0.110714         1.470584        0.113113   \n",
       "7      29.267295      0.157156         2.030930        0.097063   \n",
       "8      47.068951      3.348214         2.786811        0.595060   \n",
       "\n",
       "  param_clf__min_samples_split param_clf__n_estimators  \\\n",
       "0                            2                      50   \n",
       "1                            2                     100   \n",
       "2                            2                     200   \n",
       "3                            3                      50   \n",
       "4                            3                     100   \n",
       "5                            3                     200   \n",
       "6                            4                      50   \n",
       "7                            4                     100   \n",
       "8                            4                     200   \n",
       "\n",
       "  param_vect__max_features                                             params  \\\n",
       "0                      500  {'clf__min_samples_split': 2, 'clf__n_estimato...   \n",
       "1                      500  {'clf__min_samples_split': 2, 'clf__n_estimato...   \n",
       "2                      500  {'clf__min_samples_split': 2, 'clf__n_estimato...   \n",
       "3                      500  {'clf__min_samples_split': 3, 'clf__n_estimato...   \n",
       "4                      500  {'clf__min_samples_split': 3, 'clf__n_estimato...   \n",
       "5                      500  {'clf__min_samples_split': 3, 'clf__n_estimato...   \n",
       "6                      500  {'clf__min_samples_split': 4, 'clf__n_estimato...   \n",
       "7                      500  {'clf__min_samples_split': 4, 'clf__n_estimato...   \n",
       "8                      500  {'clf__min_samples_split': 4, 'clf__n_estimato...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0           0.255531           0.261703           0.256105           0.251323   \n",
       "1           0.259372           0.267971           0.255008           0.253090   \n",
       "2           0.262370           0.268882           0.254261           0.256976   \n",
       "3           0.260934           0.267176           0.255395           0.252899   \n",
       "4           0.257970           0.266255           0.255736           0.251952   \n",
       "5           0.260316           0.267578           0.254942           0.254343   \n",
       "6           0.258780           0.266520           0.257201           0.253050   \n",
       "7           0.254766           0.264434           0.256616           0.251844   \n",
       "8           0.257434           0.263399           0.252524           0.251068   \n",
       "\n",
       "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.252570         0.255446        0.003602                7  \n",
       "1           0.249962         0.257081        0.006241                5  \n",
       "2           0.254217         0.259341        0.005619                1  \n",
       "3           0.256447         0.258570        0.005028                2  \n",
       "4           0.252136         0.256810        0.005238                6  \n",
       "5           0.252486         0.257933        0.005481                3  \n",
       "6           0.251246         0.257359        0.005327                4  \n",
       "7           0.248266         0.255185        0.005419                8  \n",
       "8           0.247593         0.254404        0.005498                9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  296 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [500],\n",
    "              'clf__min_samples_split': [2, 3, 4],\n",
    "              'clf__n_estimators': [50, 100, 200]\n",
    "              }\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train,\n",
    "                    scorer=my_scorer, n_jobs=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more estimators improves the scores, though only slightly. Using `min_samples_split` = 3 does better than the default value of 2, so I will use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__min_samples_split': 2,\n",
       " 'clf__n_estimators': 200,\n",
       " 'vect__max_features': 500}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best hyperparameter values that were found\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.946\n",
      "precision = 0.814\n",
      "recall = 0.521\n",
      "F1 score = 0.636\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set and show the appropriate metrics\n",
    "Y_pred = cv.best_estimator_.predict(X_test)\n",
    "print_metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What were the values of these metrics for the original model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.943\n",
      "precision = 0.846\n",
      "recall = 0.46\n",
      "F1 score = 0.596\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and show metrics for the original model\n",
    "Y_pred = pipe_single.predict(X_test)\n",
    "print_metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is a bit greater for the original model, but I don't care about that too much; the precision/recall tradeoff still favors precision too much for this application. The recall and F1 score are a few percent better for my tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Improving the tuned model further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned above, while it's more appropriate than the accuracy, the F1 score is still not the right metric for this problem. What I really want is to reduce the work that human disaster response professionals have to do in reading the messages that come in to a maneagable level, while catching almost all the messages that are in a given category. In other words, for each of the categories, I want the recall to be very high, say greater than 90%, while having precision high enough that no more than half the messages the humans have to read are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My original model above did pretty well in terms of recall for the 'related' column. It would make sense to me if no messages that are unrelated (i.e. have a value of 0 in this column) have a value of 1 in any other column (since they are presumably unrelated to the disaster). I'll now check to see if this is really the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create a dataframe that has targets for unrelated messages only\n",
    "Y_unrelated = Y_train.drop(Y_train.loc[Y_train['related'] == 1].index)\n",
    "# Drop the 'related' column\n",
    "Y_unrelated.drop(columns='related', inplace=True)\n",
    "# Check whether any of these elements are nonzero\n",
    "print(sum(Y_unrelated.sum(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so my intuition is correct here. This means that it would make some sense to build a very good model to determine whether messages are related at all, then feed only messages that it predicts to be related to another model for each of the other target columns. This would already reduce number of irrelevant messages by about 25%, as that fraction of the messages are not related at all to disasters (I have to say, I would expect this fraction in the real world to be higher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few next steps I want to take to improve the model:\n",
    "\n",
    "- Build a separate classifier for the 'related' target that has extremely high recall (> 98%) and use this as an initial step to filter out irrelevant messages for the other target classification tasks.\n",
    "- Do a stratified split for each column instead of a regular train-test split; I think this is preferable since many of the targets are quite imbalanced.\n",
    "- Use the recall as my score to optimize during hyperparameter tuning. I'm not sure how much of a difference this will make, as the random forest classifiers themselves do not try to maximize recall (the use the gini criterion for splitting), but I'll give it a try. This idea might be more useful in for training a model where the metric is more customizable, like a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start by building a random forest classifier for just the 'response' task. Since the recall is my main concern, I'll use the recall as my scoring function for this optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scorer using sklearn's `make_scorer` function\n",
    "my_scorer = make_scorer(score_func, c='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do stratified shuffle split\n",
    "splits = StratifiedShuffleSplit(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.961211</td>\n",
       "      <td>0.088566</td>\n",
       "      <td>0.676990</td>\n",
       "      <td>0.018190</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__max_features': None}</td>\n",
       "      <td>0.932568</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.942611</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.944620</td>\n",
       "      <td>0.006690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.263965</td>\n",
       "      <td>0.422212</td>\n",
       "      <td>0.674495</td>\n",
       "      <td>0.017101</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'vect__max_features': 20000}</td>\n",
       "      <td>0.934720</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.941894</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.944333</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.568137</td>\n",
       "      <td>0.200245</td>\n",
       "      <td>0.664034</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>15000</td>\n",
       "      <td>{'vect__max_features': 15000}</td>\n",
       "      <td>0.934003</td>\n",
       "      <td>0.943329</td>\n",
       "      <td>0.942611</td>\n",
       "      <td>0.938307</td>\n",
       "      <td>0.956241</td>\n",
       "      <td>0.942898</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.493215</td>\n",
       "      <td>0.046136</td>\n",
       "      <td>0.656688</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'vect__max_features': 10000}</td>\n",
       "      <td>0.929699</td>\n",
       "      <td>0.939742</td>\n",
       "      <td>0.932568</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.937446</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.434960</td>\n",
       "      <td>0.880884</td>\n",
       "      <td>0.541282</td>\n",
       "      <td>0.045797</td>\n",
       "      <td>5000</td>\n",
       "      <td>{'vect__max_features': 5000}</td>\n",
       "      <td>0.927547</td>\n",
       "      <td>0.937590</td>\n",
       "      <td>0.931133</td>\n",
       "      <td>0.931851</td>\n",
       "      <td>0.938307</td>\n",
       "      <td>0.933286</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      15.961211      0.088566         0.676990        0.018190   \n",
       "1      14.263965      0.422212         0.674495        0.017101   \n",
       "2      12.568137      0.200245         0.664034        0.020906   \n",
       "3      11.493215      0.046136         0.656688        0.008081   \n",
       "4       8.434960      0.880884         0.541282        0.045797   \n",
       "\n",
       "  param_vect__max_features                         params  split0_test_score  \\\n",
       "0                     None   {'vect__max_features': None}           0.932568   \n",
       "1                    20000  {'vect__max_features': 20000}           0.934720   \n",
       "2                    15000  {'vect__max_features': 15000}           0.934003   \n",
       "3                    10000  {'vect__max_features': 10000}           0.929699   \n",
       "4                     5000   {'vect__max_features': 5000}           0.927547   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.950502           0.942611           0.946915           0.950502   \n",
       "1           0.944046           0.950502           0.941894           0.950502   \n",
       "2           0.943329           0.942611           0.938307           0.956241   \n",
       "3           0.939742           0.932568           0.939024           0.946198   \n",
       "4           0.937590           0.931133           0.931851           0.938307   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.944620        0.006690                1  \n",
       "1         0.944333        0.005909                2  \n",
       "2         0.942898        0.007463                3  \n",
       "3         0.937446        0.005800                4  \n",
       "4         0.933286        0.004083                5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  64.1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [None, 20000, 15000, 10000, 5000]}\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train['related'], \n",
    "                    scorer=my_scorer, n_jobs=7, cv=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between using 20,000 features and the full number is statistically insignificant, so I'll go with using 20,000 since it shortens training time slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.179214</td>\n",
       "      <td>0.027065</td>\n",
       "      <td>0.694166</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>None</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': None, 'vect__max_feature...</td>\n",
       "      <td>0.934720</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.941894</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.944333</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.704716</td>\n",
       "      <td>0.422246</td>\n",
       "      <td>0.668341</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>balanced</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'vect__max_f...</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.930416</td>\n",
       "      <td>0.931133</td>\n",
       "      <td>0.928981</td>\n",
       "      <td>0.935438</td>\n",
       "      <td>0.928121</td>\n",
       "      <td>0.007078</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.272680</td>\n",
       "      <td>2.079906</td>\n",
       "      <td>0.611508</td>\n",
       "      <td>0.070297</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': 'balanced_subsample', 'v...</td>\n",
       "      <td>0.911047</td>\n",
       "      <td>0.935438</td>\n",
       "      <td>0.932568</td>\n",
       "      <td>0.929699</td>\n",
       "      <td>0.932568</td>\n",
       "      <td>0.928264</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      14.179214      0.027065         0.694166        0.012902   \n",
       "1      14.704716      0.422246         0.668341        0.006235   \n",
       "2      14.272680      2.079906         0.611508        0.070297   \n",
       "\n",
       "  param_clf__class_weight param_vect__max_features  \\\n",
       "0                    None                    20000   \n",
       "1                balanced                    20000   \n",
       "2      balanced_subsample                    20000   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf__class_weight': None, 'vect__max_feature...           0.934720   \n",
       "1  {'clf__class_weight': 'balanced', 'vect__max_f...           0.914634   \n",
       "2  {'clf__class_weight': 'balanced_subsample', 'v...           0.911047   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.944046           0.950502           0.941894           0.950502   \n",
       "1           0.930416           0.931133           0.928981           0.935438   \n",
       "2           0.935438           0.932568           0.929699           0.932568   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.944333        0.005909                1  \n",
       "1         0.928121        0.007078                3  \n",
       "2         0.928264        0.008798                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  52.3 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [20000],\n",
    "             'clf__class_weight': [None, 'balanced', 'balanced_subsample']}\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train['related'], \n",
    "                    scorer=my_scorer, n_jobs=7, cv=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, using unbalanced class weights works better. I'm a bit surprised by this, although here the class imbalance is not huge (about 3:1), so maybe it's not so puzzling.\n",
    "\n",
    "I'll finish out the parameter search with a grid for a few more parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__min_samples_split</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>param_tfidf__norm</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.845285</td>\n",
       "      <td>0.057213</td>\n",
       "      <td>0.540063</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.936155</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.942611</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.941894</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.339291</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>0.539383</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.939024</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.944907</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.134381</td>\n",
       "      <td>0.064673</td>\n",
       "      <td>0.544617</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.928981</td>\n",
       "      <td>0.939742</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.941894</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.550402</td>\n",
       "      <td>0.054693</td>\n",
       "      <td>0.552346</td>\n",
       "      <td>0.008418</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.936155</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.943902</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.279853</td>\n",
       "      <td>0.082567</td>\n",
       "      <td>0.725527</td>\n",
       "      <td>0.008494</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.940459</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.041215</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>0.728314</td>\n",
       "      <td>0.009272</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.943329</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.421454</td>\n",
       "      <td>0.090901</td>\n",
       "      <td>0.726941</td>\n",
       "      <td>0.007887</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.934720</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.941894</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.944333</td>\n",
       "      <td>0.005909</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.137499</td>\n",
       "      <td>0.108512</td>\n",
       "      <td>0.726765</td>\n",
       "      <td>0.008599</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.940459</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.947059</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.767223</td>\n",
       "      <td>0.166172</td>\n",
       "      <td>1.085987</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.942611</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.948924</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29.214331</td>\n",
       "      <td>0.231285</td>\n",
       "      <td>1.088156</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.949354</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27.749552</td>\n",
       "      <td>0.173187</td>\n",
       "      <td>1.070495</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.937590</td>\n",
       "      <td>0.943329</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29.161190</td>\n",
       "      <td>0.186944</td>\n",
       "      <td>1.083252</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 2, 'clf__n_estimato...</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.954089</td>\n",
       "      <td>0.955524</td>\n",
       "      <td>0.949211</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.761964</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>0.576926</td>\n",
       "      <td>0.012334</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.943329</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.956958</td>\n",
       "      <td>0.947776</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8.921351</td>\n",
       "      <td>0.046694</td>\n",
       "      <td>0.579153</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.947489</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8.792766</td>\n",
       "      <td>0.034567</td>\n",
       "      <td>0.583713</td>\n",
       "      <td>0.010805</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.937590</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.967835</td>\n",
       "      <td>0.042811</td>\n",
       "      <td>0.580722</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.939742</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.947920</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.065056</td>\n",
       "      <td>0.049254</td>\n",
       "      <td>0.743518</td>\n",
       "      <td>0.008783</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.942611</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14.330870</td>\n",
       "      <td>0.080878</td>\n",
       "      <td>0.747073</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.955524</td>\n",
       "      <td>0.950789</td>\n",
       "      <td>0.003730</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.046065</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.746558</td>\n",
       "      <td>0.014497</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.938307</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.946485</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14.289365</td>\n",
       "      <td>0.048409</td>\n",
       "      <td>0.739115</td>\n",
       "      <td>0.011645</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.941894</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.954089</td>\n",
       "      <td>0.948494</td>\n",
       "      <td>0.004431</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>24.665713</td>\n",
       "      <td>0.037988</td>\n",
       "      <td>1.088948</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.956241</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.003240</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25.156555</td>\n",
       "      <td>0.115721</td>\n",
       "      <td>1.091303</td>\n",
       "      <td>0.009139</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.955524</td>\n",
       "      <td>0.951506</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24.576949</td>\n",
       "      <td>0.066888</td>\n",
       "      <td>1.076934</td>\n",
       "      <td>0.011251</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.954806</td>\n",
       "      <td>0.948637</td>\n",
       "      <td>0.004436</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25.168365</td>\n",
       "      <td>0.116234</td>\n",
       "      <td>1.077669</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 3, 'clf__n_estimato...</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.956958</td>\n",
       "      <td>0.949928</td>\n",
       "      <td>0.004338</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8.461475</td>\n",
       "      <td>0.042739</td>\n",
       "      <td>0.594856</td>\n",
       "      <td>0.006487</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.937590</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.946341</td>\n",
       "      <td>0.004767</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.566627</td>\n",
       "      <td>0.026570</td>\n",
       "      <td>0.589921</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.950215</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.531516</td>\n",
       "      <td>0.057406</td>\n",
       "      <td>0.599182</td>\n",
       "      <td>0.015939</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.939742</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.947202</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.639747</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.593476</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.940459</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.946628</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.289096</td>\n",
       "      <td>0.047624</td>\n",
       "      <td>0.761439</td>\n",
       "      <td>0.015129</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.940459</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.946628</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.332312</td>\n",
       "      <td>0.057724</td>\n",
       "      <td>0.759390</td>\n",
       "      <td>0.006809</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.948350</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.954089</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.950646</td>\n",
       "      <td>0.002542</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13.206234</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.757035</td>\n",
       "      <td>0.007153</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.940459</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.946198</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.947776</td>\n",
       "      <td>0.004118</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>13.286210</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.752906</td>\n",
       "      <td>0.014919</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.955524</td>\n",
       "      <td>0.948780</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>22.849928</td>\n",
       "      <td>0.026815</td>\n",
       "      <td>1.116087</td>\n",
       "      <td>0.008279</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.945481</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.952654</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>23.053727</td>\n",
       "      <td>0.082615</td>\n",
       "      <td>1.100103</td>\n",
       "      <td>0.007490</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.949785</td>\n",
       "      <td>0.955524</td>\n",
       "      <td>0.953372</td>\n",
       "      <td>0.952080</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>22.652140</td>\n",
       "      <td>0.286701</td>\n",
       "      <td>1.100176</td>\n",
       "      <td>0.031023</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.943329</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.944763</td>\n",
       "      <td>0.954806</td>\n",
       "      <td>0.949067</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>21.312478</td>\n",
       "      <td>0.350333</td>\n",
       "      <td>0.898844</td>\n",
       "      <td>0.083855</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__min_samples_split': 4, 'clf__n_estimato...</td>\n",
       "      <td>0.944046</td>\n",
       "      <td>0.950502</td>\n",
       "      <td>0.946915</td>\n",
       "      <td>0.951937</td>\n",
       "      <td>0.956958</td>\n",
       "      <td>0.950072</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        8.845285      0.057213         0.540063        0.007563   \n",
       "1        9.339291      0.139648         0.539383        0.009803   \n",
       "2        9.134381      0.064673         0.544617        0.012165   \n",
       "3        9.550402      0.054693         0.552346        0.008418   \n",
       "4       15.279853      0.082567         0.725527        0.008494   \n",
       "5       16.041215      0.143308         0.728314        0.009272   \n",
       "6       15.421454      0.090901         0.726941        0.007887   \n",
       "7       16.137499      0.108512         0.726765        0.008599   \n",
       "8       27.767223      0.166172         1.085987        0.016813   \n",
       "9       29.214331      0.231285         1.088156        0.005772   \n",
       "10      27.749552      0.173187         1.070495        0.009559   \n",
       "11      29.161190      0.186944         1.083252        0.005606   \n",
       "12       8.761964      0.028941         0.576926        0.012334   \n",
       "13       8.921351      0.046694         0.579153        0.004703   \n",
       "14       8.792766      0.034567         0.583713        0.010805   \n",
       "15       8.967835      0.042811         0.580722        0.004830   \n",
       "16      14.065056      0.049254         0.743518        0.008783   \n",
       "17      14.330870      0.080878         0.747073        0.005835   \n",
       "18      14.046065      0.020794         0.746558        0.014497   \n",
       "19      14.289365      0.048409         0.739115        0.011645   \n",
       "20      24.665713      0.037988         1.088948        0.013091   \n",
       "21      25.156555      0.115721         1.091303        0.009139   \n",
       "22      24.576949      0.066888         1.076934        0.011251   \n",
       "23      25.168365      0.116234         1.077669        0.017982   \n",
       "24       8.461475      0.042739         0.594856        0.006487   \n",
       "25       8.566627      0.026570         0.589921        0.006308   \n",
       "26       8.531516      0.057406         0.599182        0.015939   \n",
       "27       8.639747      0.034700         0.593476        0.005016   \n",
       "28      13.289096      0.047624         0.761439        0.015129   \n",
       "29      13.332312      0.057724         0.759390        0.006809   \n",
       "30      13.206234      0.042857         0.757035        0.007153   \n",
       "31      13.286210      0.019500         0.752906        0.014919   \n",
       "32      22.849928      0.026815         1.116087        0.008279   \n",
       "33      23.053727      0.082615         1.100103        0.007490   \n",
       "34      22.652140      0.286701         1.100176        0.031023   \n",
       "35      21.312478      0.350333         0.898844        0.083855   \n",
       "\n",
       "   param_clf__min_samples_split param_clf__n_estimators param_tfidf__norm  \\\n",
       "0                             2                      50                l1   \n",
       "1                             2                      50                l1   \n",
       "2                             2                      50                l2   \n",
       "3                             2                      50                l2   \n",
       "4                             2                     100                l1   \n",
       "5                             2                     100                l1   \n",
       "6                             2                     100                l2   \n",
       "7                             2                     100                l2   \n",
       "8                             2                     200                l1   \n",
       "9                             2                     200                l1   \n",
       "10                            2                     200                l2   \n",
       "11                            2                     200                l2   \n",
       "12                            3                      50                l1   \n",
       "13                            3                      50                l1   \n",
       "14                            3                      50                l2   \n",
       "15                            3                      50                l2   \n",
       "16                            3                     100                l1   \n",
       "17                            3                     100                l1   \n",
       "18                            3                     100                l2   \n",
       "19                            3                     100                l2   \n",
       "20                            3                     200                l1   \n",
       "21                            3                     200                l1   \n",
       "22                            3                     200                l2   \n",
       "23                            3                     200                l2   \n",
       "24                            4                      50                l1   \n",
       "25                            4                      50                l1   \n",
       "26                            4                      50                l2   \n",
       "27                            4                      50                l2   \n",
       "28                            4                     100                l1   \n",
       "29                            4                     100                l1   \n",
       "30                            4                     100                l2   \n",
       "31                            4                     100                l2   \n",
       "32                            4                     200                l1   \n",
       "33                            4                     200                l1   \n",
       "34                            4                     200                l2   \n",
       "35                            4                     200                l2   \n",
       "\n",
       "   param_tfidf__use_idf param_vect__max_features  \\\n",
       "0                  True                    20000   \n",
       "1                 False                    20000   \n",
       "2                  True                    20000   \n",
       "3                 False                    20000   \n",
       "4                  True                    20000   \n",
       "5                 False                    20000   \n",
       "6                  True                    20000   \n",
       "7                 False                    20000   \n",
       "8                  True                    20000   \n",
       "9                 False                    20000   \n",
       "10                 True                    20000   \n",
       "11                False                    20000   \n",
       "12                 True                    20000   \n",
       "13                False                    20000   \n",
       "14                 True                    20000   \n",
       "15                False                    20000   \n",
       "16                 True                    20000   \n",
       "17                False                    20000   \n",
       "18                 True                    20000   \n",
       "19                False                    20000   \n",
       "20                 True                    20000   \n",
       "21                False                    20000   \n",
       "22                 True                    20000   \n",
       "23                False                    20000   \n",
       "24                 True                    20000   \n",
       "25                False                    20000   \n",
       "26                 True                    20000   \n",
       "27                False                    20000   \n",
       "28                 True                    20000   \n",
       "29                False                    20000   \n",
       "30                 True                    20000   \n",
       "31                False                    20000   \n",
       "32                 True                    20000   \n",
       "33                False                    20000   \n",
       "34                 True                    20000   \n",
       "35                False                    20000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.936155   \n",
       "1   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.944763   \n",
       "2   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.928981   \n",
       "3   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.936155   \n",
       "4   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.940459   \n",
       "5   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.946915   \n",
       "6   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.934720   \n",
       "7   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.940459   \n",
       "8   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.942611   \n",
       "9   {'clf__min_samples_split': 2, 'clf__n_estimato...           0.949785   \n",
       "10  {'clf__min_samples_split': 2, 'clf__n_estimato...           0.937590   \n",
       "11  {'clf__min_samples_split': 2, 'clf__n_estimato...           0.941176   \n",
       "12  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.945481   \n",
       "13  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.945481   \n",
       "14  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.937590   \n",
       "15  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.945481   \n",
       "16  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.942611   \n",
       "17  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.944046   \n",
       "18  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.938307   \n",
       "19  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.946198   \n",
       "20  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.946198   \n",
       "21  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.947633   \n",
       "22  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.941176   \n",
       "23  {'clf__min_samples_split': 3, 'clf__n_estimato...           0.946198   \n",
       "24  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.937590   \n",
       "25  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.949785   \n",
       "26  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.939742   \n",
       "27  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.940459   \n",
       "28  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.940459   \n",
       "29  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.948350   \n",
       "30  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.940459   \n",
       "31  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.944763   \n",
       "32  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.945481   \n",
       "33  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.950502   \n",
       "34  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.943329   \n",
       "35  {'clf__min_samples_split': 4, 'clf__n_estimato...           0.944046   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.939024           0.945481           0.942611   \n",
       "1            0.939024           0.944763           0.947633   \n",
       "2            0.939742           0.946198           0.941894   \n",
       "3            0.950502           0.941176           0.945481   \n",
       "4            0.946198           0.944763           0.946198   \n",
       "5            0.943329           0.949067           0.949067   \n",
       "6            0.944046           0.950502           0.941894   \n",
       "7            0.947633           0.944046           0.949785   \n",
       "8            0.950502           0.950502           0.949067   \n",
       "9            0.946198           0.946198           0.952654   \n",
       "10           0.943329           0.948350           0.945481   \n",
       "11           0.949785           0.945481           0.954089   \n",
       "12           0.943329           0.947633           0.945481   \n",
       "13           0.946198           0.946915           0.949067   \n",
       "14           0.947633           0.949067           0.946915   \n",
       "15           0.952654           0.939742           0.952654   \n",
       "16           0.946915           0.949785           0.945481   \n",
       "17           0.951220           0.951937           0.951220   \n",
       "18           0.949067           0.949067           0.946198   \n",
       "19           0.947633           0.941894           0.952654   \n",
       "20           0.949785           0.949785           0.950502   \n",
       "21           0.951220           0.949785           0.953372   \n",
       "22           0.949067           0.950502           0.947633   \n",
       "23           0.949785           0.944763           0.951937   \n",
       "24           0.948350           0.951937           0.946198   \n",
       "25           0.946915           0.949067           0.952654   \n",
       "26           0.948350           0.949067           0.946915   \n",
       "27           0.952654           0.941176           0.947633   \n",
       "28           0.946915           0.948350           0.946915   \n",
       "29           0.948350           0.949067           0.954089   \n",
       "30           0.951937           0.949785           0.946198   \n",
       "31           0.949785           0.944046           0.949785   \n",
       "32           0.946915           0.950502           0.949785   \n",
       "33           0.951220           0.949785           0.955524   \n",
       "34           0.950502           0.951937           0.944763   \n",
       "35           0.950502           0.946915           0.951937   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.946198         0.941894        0.003823               35  \n",
       "1            0.948350         0.944907        0.003284               32  \n",
       "2            0.946915         0.940746        0.006458               36  \n",
       "3            0.946198         0.943902        0.004874               34  \n",
       "4            0.953372         0.946198        0.004158               29  \n",
       "5            0.953372         0.948350        0.003272               17  \n",
       "6            0.950502         0.944333        0.005909               33  \n",
       "7            0.953372         0.947059        0.004478               24  \n",
       "8            0.951937         0.948924        0.003284               13  \n",
       "9            0.951937         0.949354        0.002745                9  \n",
       "10           0.952654         0.945481        0.005032               31  \n",
       "11           0.955524         0.949211        0.005337               10  \n",
       "12           0.956958         0.947776        0.004789               20  \n",
       "13           0.949785         0.947489        0.001661               22  \n",
       "14           0.949785         0.946198        0.004422               29  \n",
       "15           0.949067         0.947920        0.004878               18  \n",
       "16           0.953372         0.947633        0.003686               21  \n",
       "17           0.955524         0.950789        0.003730                3  \n",
       "18           0.949785         0.946485        0.004271               27  \n",
       "19           0.954089         0.948494        0.004431               16  \n",
       "20           0.956241         0.950502        0.003240                5  \n",
       "21           0.955524         0.951506        0.002745                2  \n",
       "22           0.954806         0.948637        0.004436               15  \n",
       "23           0.956958         0.949928        0.004338                8  \n",
       "24           0.947633         0.946341        0.004767               28  \n",
       "25           0.952654         0.950215        0.002204                6  \n",
       "26           0.951937         0.947202        0.004073               23  \n",
       "27           0.951220         0.946628        0.005024               25  \n",
       "28           0.950502         0.946628        0.003352               25  \n",
       "29           0.953372         0.950646        0.002542                4  \n",
       "30           0.950502         0.947776        0.004118               19  \n",
       "31           0.955524         0.948780        0.004148               14  \n",
       "32           0.952654         0.949067        0.002567               11  \n",
       "33           0.953372         0.952080        0.002099                1  \n",
       "34           0.954806         0.949067        0.004352               11  \n",
       "35           0.956958         0.950072        0.004413                7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  459 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [20000], \n",
    "              'clf__min_samples_split': [2, 3, 4], \n",
    "              'clf__n_estimators': [50, 100, 200],\n",
    "              'tfidf__norm': ['l1', 'l2'],\n",
    "              'tfidf__use_idf': [True, False]\n",
    "             }\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train['related'], \n",
    "                    scorer=my_scorer, n_jobs=7, cv=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__min_samples_split': 4,\n",
       " 'clf__n_estimators': 200,\n",
       " 'tfidf__norm': 'l1',\n",
       " 'tfidf__use_idf': False,\n",
       " 'vect__max_features': 20000}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.825\n",
      "precision = 0.836\n",
      "recall = 0.96\n",
      "F1 score = 0.894\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set and show the appropriate metrics\n",
    "Y_pred = cv.best_estimator_.predict(X_test)\n",
    "print_metrics(Y_test['related'], Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good recall and still fairly good precision. Now I'll see what happens with a highly imbalanced category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00411680755296959"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the fraction of rows in the 'offer' class\n",
    "sum(Y_train['offer'] == 1)/len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 0.4% of messages in the training set are in the 'offer' class. My initial pipeline identified no 'offer' messages in the test set - a recall of zero (see the classification report above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.403173</td>\n",
       "      <td>0.055678</td>\n",
       "      <td>0.439689</td>\n",
       "      <td>0.013271</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vect__max_features': None}</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       4.403173      0.055678         0.439689        0.013271   \n",
       "\n",
       "  param_vect__max_features                        params  split0_test_score  \\\n",
       "0                     None  {'vect__max_features': None}                0.0   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0              0.0             0.0                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  8.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [None]}\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_single, parameters, X_train, Y_train['offer'], \n",
    "                    scorer=my_scorer, n_jobs=7, cv=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no luck, it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Looking to donate new ( several-year shelf life , sealed ) non-prescription medications to any organization involved in or impacted by Hurricane Sandy . Red Cross was contacted by a community member whose work department had collected money and purchased these items . Donation approximately fills a 10-ream paper box . Products are NOT used or near expiration .',\n",
       " '45 geminies (Inflatable rubber boats) along with out board motors and other diving equipment have been dispatched with the diving team.',\n",
       " 'It concentrated on two areas: firstly, the rehabilitation of local health, water and sanitation facilities affected by the flooding; and secondly, on increasing the future capacity of the most vulnerable communities nationwide through community-based first aid and disaster preparedness and response programmes, anchored in the development of stronger National Society branches and district and provincial organizational structures.',\n",
       " 'Out here helping this community .. . The things u see is incredible .. Keep these people in ur prayers .. . #Sandy http : //t.co/zTFbQjht',\n",
       " 'I can cook and would be happy to try to provide meals or serve in a soup kitchen .',\n",
       " 'How can we help the victims at Les Cayes?',\n",
       " \"I can also cook food or buy non perishable food and hygiene products . The site only let 's me pick one ! I do have clothes to give as well . Whatever is most needed or all .\",\n",
       " 'I can distribute food and/or goods , cook , provide Reiki ( relaxation therapy ) , check on residents , organize , provide graphic design work',\n",
       " 'To provide access to communications networks on a shared basis by installing and maintaining the network to facilitate the provision of humanitarian assistance, and hire appropriately qualified personnel to keep it running until the operation is declared complete.',\n",
       " 'Fritz Institute is a non-profit organization dedicated to addressing complex operational challenges in the delivery of humanitarian aid to people and communities in need worldwide.',\n",
       " 'International rescue teams with sniffer dogs and specialist equipment began arriving and setting up field hospitals.',\n",
       " 'The flood victims will be provided with rice, oil, household utensils and cash assistance.',\n",
       " 'We can donate clothing , towels , toys , and food',\n",
       " 'Individuals and organizations wishing to do so may go right down to the storm-hit towns, wards and villages of their choice so that the donation operation can be carried out more effectively.',\n",
       " 'She said efforts were underway to ensure shelters had adequate stocks of safe drinking water, especially bottled water, to check the spread of disease.',\n",
       " \"Mercy-USA for Aid and Development (M-USA) is collecting funds to aid the survivors of the deadly earthquakes that struck Indonesia's island of Sumatra on Tuesday, March 6.\",\n",
       " 'Various levels of governments have immediately launched emergency preparedness plans and taken effective measures to assist the affected population.',\n",
       " 'SOUTH KOREA: Raises aid to $2 million, may send military cargo plane to move aid workers and supplies.',\n",
       " 'Following the attack on Ngouboua village by armed militants from Nigeria in February, we have been also providing food to the families who lost their homes in the fire, during the attack.',\n",
       " 'LONDON, March 5 (AFP) - Military planes and helicopters took to the air Sunday across the world as the international community stepped up its aid efforts to the 1.5 million people hit by deadly floods in Mozambique.']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[Y_train[Y_train['offer'] == 1].index].tolist()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are some issues with data labeling here, at least in my estimation. To me, the 'offer' category should indicate messages coming in that are direct offers to help, such as, 'We can donate clothing , towels , toys , and food'. A lot of the messages that are labeled as being in this category are news stories, information about aid organizations, or other things. In my estimation, only 6 of the first 20 messages in this category in the training set are direct offers, and only a few more (~10 of 20 total) are relevant at all. This obviously makes it difficult to train an ML model well that will pick many of these up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the inability to use my custom scorer directly as a metric for training, I'm skeptical that I can do much better with random forests. Perhaps a neural network approach would work better; I will have to look into this further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now, I do want to get to a final form for my random forest model. I could optimize the hyperparameters separately for each column, doing 35 grid searches, but that would take a while and would produce a set of models that I'm not sure how to pickle together as one model. Instead, I'll go back to using `MultiOutputClassifier()`, building a random forest classifier for each target but doing just one grid search to find the optimal set of hyperparameters overall. What do I mean by \"optimal overall?\" I mean the set such that the score I choose is maximized. I will choose to score using only the recall for each column, and weight the first column ('related') at 50% and the rest equally, since the first column is the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use a grid that has 96 combinations of hyperparameters. I expect it to take about an hour to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the scorer using sklearn's `make_scorer` function\n",
    "weights = [0.5]\n",
    "weights += [(0.5/Y_test.shape[1]) for i in range(Y_test.shape[1])]\n",
    "my_scorer = make_scorer(score_func, c='recall', weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf__class_weight</th>\n",
       "      <th>param_clf__min_samples_split</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>param_tfidf__norm</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.457077</td>\n",
       "      <td>0.336125</td>\n",
       "      <td>3.947862</td>\n",
       "      <td>0.327742</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__min_samples_...</td>\n",
       "      <td>0.152090</td>\n",
       "      <td>0.158759</td>\n",
       "      <td>0.145297</td>\n",
       "      <td>0.154971</td>\n",
       "      <td>0.148379</td>\n",
       "      <td>0.151899</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49.035994</td>\n",
       "      <td>1.802262</td>\n",
       "      <td>2.694259</td>\n",
       "      <td>0.226287</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__min_samples_...</td>\n",
       "      <td>0.154060</td>\n",
       "      <td>0.156453</td>\n",
       "      <td>0.149732</td>\n",
       "      <td>0.152156</td>\n",
       "      <td>0.152374</td>\n",
       "      <td>0.152955</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.342997</td>\n",
       "      <td>0.375318</td>\n",
       "      <td>3.419655</td>\n",
       "      <td>0.368266</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>True</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__min_samples_...</td>\n",
       "      <td>0.163283</td>\n",
       "      <td>0.163179</td>\n",
       "      <td>0.161791</td>\n",
       "      <td>0.163216</td>\n",
       "      <td>0.162497</td>\n",
       "      <td>0.162793</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.198780</td>\n",
       "      <td>0.353540</td>\n",
       "      <td>3.401619</td>\n",
       "      <td>0.351545</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__min_samples_...</td>\n",
       "      <td>0.149676</td>\n",
       "      <td>0.156547</td>\n",
       "      <td>0.150081</td>\n",
       "      <td>0.152274</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>0.151622</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.143893</td>\n",
       "      <td>0.245518</td>\n",
       "      <td>2.836328</td>\n",
       "      <td>0.126184</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>l1</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': None, 'clf__min_samples_...</td>\n",
       "      <td>0.154009</td>\n",
       "      <td>0.156471</td>\n",
       "      <td>0.151097</td>\n",
       "      <td>0.153217</td>\n",
       "      <td>0.150486</td>\n",
       "      <td>0.153056</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>77.357745</td>\n",
       "      <td>9.482798</td>\n",
       "      <td>4.995969</td>\n",
       "      <td>0.905136</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__min_sa...</td>\n",
       "      <td>0.194445</td>\n",
       "      <td>0.151170</td>\n",
       "      <td>0.190541</td>\n",
       "      <td>0.189361</td>\n",
       "      <td>0.210147</td>\n",
       "      <td>0.187133</td>\n",
       "      <td>0.019458</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>76.807281</td>\n",
       "      <td>6.108728</td>\n",
       "      <td>4.942798</td>\n",
       "      <td>0.399872</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>True</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__min_sa...</td>\n",
       "      <td>0.163666</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.171389</td>\n",
       "      <td>0.159780</td>\n",
       "      <td>0.173850</td>\n",
       "      <td>0.163588</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>76.959795</td>\n",
       "      <td>10.067557</td>\n",
       "      <td>4.883833</td>\n",
       "      <td>0.717175</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__min_sa...</td>\n",
       "      <td>0.197714</td>\n",
       "      <td>0.151717</td>\n",
       "      <td>0.181850</td>\n",
       "      <td>0.178597</td>\n",
       "      <td>0.224074</td>\n",
       "      <td>0.186790</td>\n",
       "      <td>0.023795</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>74.256747</td>\n",
       "      <td>8.996575</td>\n",
       "      <td>7.711923</td>\n",
       "      <td>2.383295</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>20000</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__min_sa...</td>\n",
       "      <td>0.190609</td>\n",
       "      <td>0.152042</td>\n",
       "      <td>0.189602</td>\n",
       "      <td>0.186007</td>\n",
       "      <td>0.207077</td>\n",
       "      <td>0.185067</td>\n",
       "      <td>0.018040</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>60.580335</td>\n",
       "      <td>11.849766</td>\n",
       "      <td>3.763059</td>\n",
       "      <td>1.215582</td>\n",
       "      <td>balanced</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>l2</td>\n",
       "      <td>False</td>\n",
       "      <td>10000</td>\n",
       "      <td>{'clf__class_weight': 'balanced', 'clf__min_sa...</td>\n",
       "      <td>0.168345</td>\n",
       "      <td>0.152075</td>\n",
       "      <td>0.176055</td>\n",
       "      <td>0.160517</td>\n",
       "      <td>0.174938</td>\n",
       "      <td>0.166386</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       48.457077      0.336125         3.947862        0.327742   \n",
       "1       49.035994      1.802262         2.694259        0.226287   \n",
       "2       44.342997      0.375318         3.419655        0.368266   \n",
       "3       54.198780      0.353540         3.401619        0.351545   \n",
       "4       52.143893      0.245518         2.836328        0.126184   \n",
       "..            ...           ...              ...             ...   \n",
       "91      77.357745      9.482798         4.995969        0.905136   \n",
       "92      76.807281      6.108728         4.942798        0.399872   \n",
       "93      76.959795     10.067557         4.883833        0.717175   \n",
       "94      74.256747      8.996575         7.711923        2.383295   \n",
       "95      60.580335     11.849766         3.763059        1.215582   \n",
       "\n",
       "   param_clf__class_weight param_clf__min_samples_split  \\\n",
       "0                     None                            2   \n",
       "1                     None                            2   \n",
       "2                     None                            2   \n",
       "3                     None                            2   \n",
       "4                     None                            2   \n",
       "..                     ...                          ...   \n",
       "91                balanced                            3   \n",
       "92                balanced                            3   \n",
       "93                balanced                            3   \n",
       "94                balanced                            3   \n",
       "95                balanced                            3   \n",
       "\n",
       "   param_clf__n_estimators param_tfidf__norm param_tfidf__use_idf  \\\n",
       "0                      100                l1                 True   \n",
       "1                      100                l1                 True   \n",
       "2                      100                l1                 True   \n",
       "3                      100                l1                False   \n",
       "4                      100                l1                False   \n",
       "..                     ...               ...                  ...   \n",
       "91                     200                l2                 True   \n",
       "92                     200                l2                 True   \n",
       "93                     200                l2                False   \n",
       "94                     200                l2                False   \n",
       "95                     200                l2                False   \n",
       "\n",
       "   param_vect__max_features  \\\n",
       "0                      None   \n",
       "1                     20000   \n",
       "2                     10000   \n",
       "3                      None   \n",
       "4                     20000   \n",
       "..                      ...   \n",
       "91                    20000   \n",
       "92                    10000   \n",
       "93                     None   \n",
       "94                    20000   \n",
       "95                    10000   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'clf__class_weight': None, 'clf__min_samples_...           0.152090   \n",
       "1   {'clf__class_weight': None, 'clf__min_samples_...           0.154060   \n",
       "2   {'clf__class_weight': None, 'clf__min_samples_...           0.163283   \n",
       "3   {'clf__class_weight': None, 'clf__min_samples_...           0.149676   \n",
       "4   {'clf__class_weight': None, 'clf__min_samples_...           0.154009   \n",
       "..                                                ...                ...   \n",
       "91  {'clf__class_weight': 'balanced', 'clf__min_sa...           0.194445   \n",
       "92  {'clf__class_weight': 'balanced', 'clf__min_sa...           0.163666   \n",
       "93  {'clf__class_weight': 'balanced', 'clf__min_sa...           0.197714   \n",
       "94  {'clf__class_weight': 'balanced', 'clf__min_sa...           0.190609   \n",
       "95  {'clf__class_weight': 'balanced', 'clf__min_sa...           0.168345   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.158759           0.145297           0.154971   \n",
       "1            0.156453           0.149732           0.152156   \n",
       "2            0.163179           0.161791           0.163216   \n",
       "3            0.156547           0.150081           0.152274   \n",
       "4            0.156471           0.151097           0.153217   \n",
       "..                ...                ...                ...   \n",
       "91           0.151170           0.190541           0.189361   \n",
       "92           0.149254           0.171389           0.159780   \n",
       "93           0.151717           0.181850           0.178597   \n",
       "94           0.152042           0.189602           0.186007   \n",
       "95           0.152075           0.176055           0.160517   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.148379         0.151899        0.004744               72  \n",
       "1            0.152374         0.152955        0.002228               68  \n",
       "2            0.162497         0.162793        0.000576               35  \n",
       "3            0.149533         0.151622        0.002653               73  \n",
       "4            0.150486         0.153056        0.002147               66  \n",
       "..                ...              ...             ...              ...  \n",
       "91           0.210147         0.187133        0.019458                6  \n",
       "92           0.173850         0.163588        0.008788               32  \n",
       "93           0.224074         0.186790        0.023795                7  \n",
       "94           0.207077         0.185067        0.018040                8  \n",
       "95           0.174938         0.166386        0.009052               23  \n",
       "\n",
       "[96 rows x 19 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime =  5.01e+03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Make a dictionary with the parameter values to try\n",
    "parameters = {'vect__max_features': [None, 20000, 10000],\n",
    "             'clf__class_weight': [None, 'balanced'],\n",
    "              'clf__min_samples_split': [2, 3], \n",
    "              'clf__n_estimators': [100, 200],\n",
    "              'tfidf__norm': ['l1', 'l2'],\n",
    "              'tfidf__use_idf': [True, False]\n",
    "             }\n",
    "\n",
    "# Do the grid search\n",
    "cv = do_grid_search(pipe_multi, parameters, X_train, Y_train, \n",
    "                    scorer=my_scorer, n_jobs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually took more like 1.5 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__class_weight': 'balanced',\n",
       " 'clf__min_samples_split': 3,\n",
       " 'clf__n_estimators': 100,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__max_features': None}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best hyperparameter values found\n",
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(stop_words=['her', 'll', 're', 'all', 'some', 'will', 'she',\n",
       "                               'that', 'when', 'than', 'shan', 'your', 'down',\n",
       "                               'off', 'those', 't', 'yourselves', 'during', 'the',\n",
       "                               'not', 'have', 'we', 'an', 'below', 'out', 'can',\n",
       "                               'should', 'into', 'myself', 'themselves', ...],\n",
       "                   tokenizer=<function tokenize at 0x15997c1f0>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42)))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(stop_words=['her', 'll', 're', 'all', 'some', 'will', 'she',\n",
       "                             'that', 'when', 'than', 'shan', 'your', 'down',\n",
       "                             'off', 'those', 't', 'yourselves', 'during', 'the',\n",
       "                             'not', 'have', 'we', 'an', 'below', 'out', 'can',\n",
       "                             'should', 'into', 'myself', 'themselves', ...],\n",
       "                 tokenizer=<function tokenize at 0x15997c1f0>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42)),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': ['her',\n",
       "  'll',\n",
       "  're',\n",
       "  'all',\n",
       "  'some',\n",
       "  'will',\n",
       "  'she',\n",
       "  'that',\n",
       "  'when',\n",
       "  'than',\n",
       "  'shan',\n",
       "  'your',\n",
       "  'down',\n",
       "  'off',\n",
       "  'those',\n",
       "  't',\n",
       "  'yourselves',\n",
       "  'during',\n",
       "  'the',\n",
       "  'not',\n",
       "  'have',\n",
       "  'we',\n",
       "  'an',\n",
       "  'below',\n",
       "  'out',\n",
       "  'can',\n",
       "  'should',\n",
       "  'into',\n",
       "  'myself',\n",
       "  'themselves',\n",
       "  'if',\n",
       "  's',\n",
       "  'hasn',\n",
       "  'just',\n",
       "  'being',\n",
       "  'up',\n",
       "  'his',\n",
       "  'd',\n",
       "  'is',\n",
       "  'am',\n",
       "  'this',\n",
       "  'other',\n",
       "  'where',\n",
       "  'against',\n",
       "  'only',\n",
       "  'with',\n",
       "  'ours',\n",
       "  'whom',\n",
       "  'm',\n",
       "  'these',\n",
       "  'didn',\n",
       "  'doe',\n",
       "  'which',\n",
       "  'me',\n",
       "  'our',\n",
       "  'i',\n",
       "  'had',\n",
       "  'both',\n",
       "  'own',\n",
       "  'been',\n",
       "  'few',\n",
       "  'no',\n",
       "  'how',\n",
       "  'above',\n",
       "  'hadn',\n",
       "  'through',\n",
       "  'under',\n",
       "  'won',\n",
       "  'until',\n",
       "  'itself',\n",
       "  've',\n",
       "  'most',\n",
       "  'mightn',\n",
       "  'between',\n",
       "  'aren',\n",
       "  'then',\n",
       "  'them',\n",
       "  'a',\n",
       "  'don',\n",
       "  'or',\n",
       "  'for',\n",
       "  'at',\n",
       "  'and',\n",
       "  'each',\n",
       "  'haven',\n",
       "  'they',\n",
       "  'shouldn',\n",
       "  'now',\n",
       "  'there',\n",
       "  'in',\n",
       "  'theirs',\n",
       "  'why',\n",
       "  'from',\n",
       "  'further',\n",
       "  'once',\n",
       "  'very',\n",
       "  'you',\n",
       "  'yours',\n",
       "  'what',\n",
       "  'it',\n",
       "  'of',\n",
       "  'ourselves',\n",
       "  'more',\n",
       "  'nor',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'any',\n",
       "  'isn',\n",
       "  'needn',\n",
       "  'mustn',\n",
       "  'be',\n",
       "  'while',\n",
       "  'wasn',\n",
       "  'himself',\n",
       "  'their',\n",
       "  'because',\n",
       "  'before',\n",
       "  'but',\n",
       "  'my',\n",
       "  'again',\n",
       "  'o',\n",
       "  'such',\n",
       "  'same',\n",
       "  'doesn',\n",
       "  'do',\n",
       "  'did',\n",
       "  'too',\n",
       "  'couldn',\n",
       "  'wouldn',\n",
       "  'about',\n",
       "  'herself',\n",
       "  'he',\n",
       "  'are',\n",
       "  'having',\n",
       "  'to',\n",
       "  'by',\n",
       "  'him',\n",
       "  'were',\n",
       "  'over',\n",
       "  'yourself',\n",
       "  'after',\n",
       "  'on',\n",
       "  'so',\n",
       "  'here',\n",
       "  'ha',\n",
       "  'wa',\n",
       "  'doing',\n",
       "  'ma',\n",
       "  'hers',\n",
       "  'who',\n",
       "  'weren'],\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': 42,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(random_state=42),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameters found are balanced class weights and 3 for 'min_samples_split' for the random forests; otherwise they are the default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the results on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.939\n",
      "precision = 0.719\n",
      "recall = 0.542\n",
      "F1 score = 0.618\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set and show the appropriate metrics\n",
    "Y_pred = cv.best_estimator_.predict(X_test)\n",
    "print_metrics(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mrelated:\u001b[0m\n",
      "recall = 0.989; precision = 0.788; class ratio = 3.23\n",
      "------------------------------------------------------------\n",
      "\u001b[1mrequest:\u001b[0m\n",
      "recall = 0.392; precision = 0.86; class ratio = 0.206\n",
      "------------------------------------------------------------\n",
      "\u001b[1moffer:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.00554\n",
      "------------------------------------------------------------\n",
      "\u001b[1maid_related:\u001b[0m\n",
      "recall = 0.92; precision = 0.563; class ratio = 0.708\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmedical_help:\u001b[0m\n",
      "recall = 0.0712; precision = 0.677; class ratio = 0.0859\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmedical_products:\u001b[0m\n",
      "recall = 0.104; precision = 0.719; class ratio = 0.0533\n",
      "------------------------------------------------------------\n",
      "\u001b[1msearch_and_rescue:\u001b[0m\n",
      "recall = 0.0317; precision = 0.583; class ratio = 0.0291\n",
      "------------------------------------------------------------\n",
      "\u001b[1msecurity:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0207\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmilitary:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0329\n",
      "------------------------------------------------------------\n",
      "\u001b[1mwater:\u001b[0m\n",
      "recall = 0.498; precision = 0.781; class ratio = 0.0675\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfood:\u001b[0m\n",
      "recall = 0.649; precision = 0.775; class ratio = 0.126\n",
      "------------------------------------------------------------\n",
      "\u001b[1mshelter:\u001b[0m\n",
      "recall = 0.391; precision = 0.788; class ratio = 0.0991\n",
      "------------------------------------------------------------\n",
      "\u001b[1mclothing:\u001b[0m\n",
      "recall = 0.163; precision = 0.645; class ratio = 0.016\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmoney:\u001b[0m\n",
      "recall = 0.00535; precision = 1; class ratio = 0.0245\n",
      "------------------------------------------------------------\n",
      "\u001b[1mmissing_people:\u001b[0m\n",
      "recall = 0.0127; precision = 0.5; class ratio = 0.0102\n",
      "------------------------------------------------------------\n",
      "\u001b[1mrefugees:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0328\n",
      "------------------------------------------------------------\n",
      "\u001b[1mdeath:\u001b[0m\n",
      "recall = 0.142; precision = 0.911; class ratio = 0.0482\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_aid:\u001b[0m\n",
      "recall = 0.0177; precision = 0.643; class ratio = 0.15\n",
      "------------------------------------------------------------\n",
      "\u001b[1minfrastructure_related:\u001b[0m\n",
      "recall = 0.0117; precision = 0.214; class ratio = 0.07\n",
      "------------------------------------------------------------\n",
      "\u001b[1mtransport:\u001b[0m\n",
      "recall = 0.0519; precision = 0.643; class ratio = 0.0465\n",
      "------------------------------------------------------------\n",
      "\u001b[1mbuildings:\u001b[0m\n",
      "recall = 0.139; precision = 0.75; class ratio = 0.0584\n",
      "------------------------------------------------------------\n",
      "\u001b[1melectricity:\u001b[0m\n",
      "recall = 0.0464; precision = 0.636; class ratio = 0.0197\n",
      "------------------------------------------------------------\n",
      "\u001b[1mtools:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0067\n",
      "------------------------------------------------------------\n",
      "\u001b[1mhospitals:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0109\n",
      "------------------------------------------------------------\n",
      "\u001b[1mshops:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.00437\n",
      "------------------------------------------------------------\n",
      "\u001b[1maid_centers:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0115\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_infrastructure:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0458\n",
      "------------------------------------------------------------\n",
      "\u001b[1mweather_related:\u001b[0m\n",
      "recall = 0.798; precision = 0.721; class ratio = 0.393\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfloods:\u001b[0m\n",
      "recall = 0.0386; precision = 0.96; class ratio = 0.0864\n",
      "------------------------------------------------------------\n",
      "\u001b[1mstorm:\u001b[0m\n",
      "recall = 0.058; precision = 0.827; class ratio = 0.105\n",
      "------------------------------------------------------------\n",
      "\u001b[1mfire:\u001b[0m\n",
      "recall = 0.0114; precision = 0.2; class ratio = 0.0114\n",
      "------------------------------------------------------------\n",
      "\u001b[1mearthquake:\u001b[0m\n",
      "recall = 0.682; precision = 0.874; class ratio = 0.105\n",
      "------------------------------------------------------------\n",
      "\u001b[1mcold:\u001b[0m\n",
      "recall = 0.0115; precision = 0.667; class ratio = 0.0228\n",
      "------------------------------------------------------------\n",
      "\u001b[1mother_weather:\u001b[0m\n",
      "recall = 0; precision = 0; class ratio = 0.0537\n",
      "------------------------------------------------------------\n",
      "\u001b[1mdirect_report:\u001b[0m\n",
      "recall = 0.245; precision = 0.845; class ratio = 0.248\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show the recall and precision for test set predictions for each target\n",
    "# Also show the class imbalance, the ratio of instances in class 1 to class 0\n",
    "for i, col in enumerate(Y_test.columns):\n",
    "    print(f'\\033[1m{col}:\\033[0m')\n",
    "    report = classification_report(Y_test[col], Y_pred[:, i], \n",
    "                                   zero_division=0, output_dict=True)\n",
    "    try:\n",
    "        recall = report['1']['recall']\n",
    "        precision = report['1']['precision']\n",
    "        frac = report['1']['support']/report['0']['support']\n",
    "    except KeyError:\n",
    "        # In this case there are no instances in Class 1\n",
    "        recall, precision, frac = 0, 0, 0\n",
    "    print(f'recall = {recall:.3g}; precision = {precision:.3g}; class ratio = {frac:.3g}')\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the recall and precision, I have shown the class imbalance for each target, defined as the ratio of the number of instances in class 1 to the number of instances in class 0. For most of the targets, this is significantly less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is great on the test set for the 'related' target, at 98.9%, though precision is only about 80% (not great here since about 76% of the instances in this target are in class 1, meaning that this classifier basically assigns class 1 to all the messages). Though not ideal, at least it's not missing a bunch of disaster-related messages. For a lot of the other targets, the recall is quite poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List the recalls for the other targets and make a histogram\n",
    "recall_list = []\n",
    "for i, col in enumerate(Y_test.columns):\n",
    "    if i == 0:\n",
    "        # Skip the 'related' target\n",
    "        continue\n",
    "    report = classification_report(Y_test[col], Y_pred[:, i], \n",
    "                                   zero_division=0, output_dict=True)\n",
    "    try:\n",
    "        recall = report['1']['recall']\n",
    "    except KeyError:\n",
    "        # In this case there are no instances in Class 1\n",
    "        recall = 0\n",
    "    recall_list.append(recall)\n",
    "\n",
    "plt.hist(recall_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the majority of the 34 other targets, the recall is less than 10%, leaving a lot to be desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21047    A young army doctor running a field clinic set...\n",
       "12462     The storm is hitting us directly, in a hole I go\n",
       "25670    In this capacity, Ly covers the entire rural c...\n",
       "13913    After the cyclone of 18 October and the super ...\n",
       "14720    The Save and the Buzi rivers are reportedly ri...\n",
       "                               ...                        \n",
       "9563        I need some information about the earthquake. \n",
       "2894     ALAS WE ARE DYING OF HUNGER AND THIRST. WE ARE...\n",
       "20602    Key priorities include identifying and verifyi...\n",
       "26125    Yudhoyone, who swept to power last September o...\n",
       "13565    This is also the opportunity to develop a bett...\n",
       "Name: message, Length: 7809, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = cv.best_estimator_.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'water', 'food', 'shelter', 'clothing', 'money', 'missing_people',\n",
       "       'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have one more idea for improving predictions using this pipeline: I can look at the precion-recall curves and see if I should change the decision threshold for some of the classifiers to favor higher recall at the cost of some precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I found that this does not work. For example, for the 'request' column, here is a plot of the precision-recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mrequest:\u001b[0m fraction_1 = 0.171\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the precision-recall curve for the 'request' column (column 1)\n",
    "col = 'request'\n",
    "num = 1\n",
    "report = classification_report(Y_test[col], Y_pred[:, num], \n",
    "                               zero_division=0, output_dict=True)\n",
    "frac = report['1']['support']/(report['0']['support'] + report['1']['support'])\n",
    "print(f'\\033[1m{col}:\\033[0m fraction_1 = {frac:.3g}')\n",
    "precision, recall, _ = precision_recall_curve(Y_test[col], Y_pred[:, num])\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 'fraction_1' is the fraction of test set instances in class 1. The recall from my predictor above is 0.392; observe that unless I'm willing to use a trivial classifier that marks all instances '1', this recall is the best I can do. In general this is true if there are no more than 3 points on the precision-recall curve. Are there any with more than three points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of lists, where each inner list contains the precisions of one\n",
    "# of the classifiers\n",
    "precision_lists = []\n",
    "# Same as the previous list, but for the recall\n",
    "recall_lists = []\n",
    "for num, col in enumerate(Y_test.columns):\n",
    "    report = classification_report(Y_test[col], Y_pred[:, num], \n",
    "                                   zero_division=0, output_dict=True)\n",
    "    precisions, recalls, _ = precision_recall_curve(Y_test[col], Y_pred[:, num])\n",
    "    precision_lists.append(precisions)\n",
    "    recall_lists.append(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any of the recall lists have length greater than 3\n",
    "for num, col in enumerate(Y_test.columns):\n",
    "    if len(recall_lists[num]) > 3:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, no luck with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for further study: \n",
    "- Try Word2Vec instead of CountVectorizer for features.\n",
    "- Could try using XGBoost instead of random forests, though I'm not sure if this will lead to significant improvement.\n",
    "- Try using keras-tcn (temporal CNN) for modeling intead of a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to export my model for use in a machine learning pipeline. I'll use Python's `pickle` module. Another popular option is `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the model and save to 'final_model.pkl'\n",
    "with open('final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(cv.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure reloading the model works\n",
    "with open('final_model.pkl' , 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "# Check whether the predictions match\n",
    "np.where((cv.best_estimator_.predict(X_test) == loaded_model.predict(X_test)) is False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, the predictions are the same, i.e. the model has been saved and then reloaded correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
